{
    "url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)",
    "scraped_data": {
        "paragraphs": [
            "Atransformeris adeep learningarchitecture developed by researchers atGoogleand based on the multiheadattentionmechanism proposed in the paper Attention Is All You NeedText is converted to numerical representations calledtokens and each token is converted into a vector via lookup from aword embeddingtableAt each layer each token is thencontextualizedwithin the scope of the context window with other tokens via a parallel multihead attention mechanism allowing the signal for key tokens to be amplified and less important tokens to be diminished",
            "Transformers have the advantage of having no recurrent units therefore requiring less training time than earlierrecurrent neural architectures such aslong shortterm memoryLater variations have been widely adopted for traininglarge language models on large datasets such as theWikipediacorpusandCommon Crawl",
            "Transformers were first developed as an improvement over previous architectures formachine translationbut have found many applications since They are used in largescalenatural language processingcomputer visionreinforcement learningaudiomultimodal learningroboticsand even playingchessIt has also led to the development ofpretrained systems such asgenerative pretrained transformersandBERT",
            "For many years sequence modelling and generation was done by using plainrecurrent neural networks A wellcited early example was theElman network In theory the information from one token can propagate arbitrarily far down the sequence but in practice thevanishinggradient problemleaves the models state at the end of a long sentence without precise extractable information about preceding tokens",
            "A key breakthrough wasLSTMa RNN which used various innovations to overcome the vanishing gradient problem allowing efficient learning of longsequence modelling One key innovation was the use of anattention mechanismwhich used neurons that multiply the outputs of other neurons socalledmultiplicative unitsNeural networks using multiplicative units were later calledsigmapi networksorhigherorder networksLSTM became the standard architecture for long sequence modelling until the publication of Transformers However LSTM still used sequential processing like most other RNNsSpecifically RNNs operate one token at a time from first to last they cannot operate in parallel over all tokens in a sequence",
            "Modern Transformers overcome this problem but unlike RNNs they require computation time that isquadraticin the size of the context window The linearly scalingfast weightcontroller learns to compute a weight matrix for further processing depending on the inputOne of its two networks has fast weights or dynamic links A slow neural network learns by gradient descent to generate keys and values for computing the weight changes of the fast neural network which computes answers to queriesThis was later shown to be equivalent to the unnormalized linear Transformer",
            "The idea of encoderdecoder sequence transduction had been developed in the early s The papers most commonly cited as the originators that produced seqseq are two concurrently published papers from",
            "A Mparameter model for machine translation uses twolong shortterm memoriesIts architecture consists of two parts Theencoderis an LSTM that takes in a sequence of tokens and turns it into a vector Thedecoderis another LSTM that converts the vector into a sequence of tokens Similarly another Mparameter model usedgated recurrent units instead of LSTMLater research showed that GRUs are neither better nor worse than LSTMs for seqseq",
            "These early seqseq models had no attention mechanism and the state vector is accessible only after thelastword of the source text was processed Although in theory such a vector retains the information about the whole original sentence in practice the information is poorly preserved This is because the input is processed sequentially by one recurrent network into afixedsize output vector which is then processed by another recurrent network into an output If the input is long then the output vector would not be able to contain all relevant information degrading the output As evidence reversing the input sentence improved seqseq translation",
            "TheRNNsearchmodel introduced an attention mechanism to seqseq for machine translation to solve the bottleneck problem allowing the model to process longdistance dependencies more easily The name is because it emulates searching through a source sentence during decoding a translation",
            "The relative performances were compared between global and local attention model architectures for machine translation finding that mixed attention had higher quality than global attention while local attention reduced translation time",
            "In Google Translatewas revamped toGoogle Neural Machine Translation which replaced the previous model based onstatistical machine translation The new model was a seqseq model where the encoder and the decoder were both layers of bidirectional LSTMIt took nine months to develop and it outperformed the statistical approach which took ten years to develop",
            "Seqseq models with attention still suffered from the same issue with recurrent networks which is that they are hard toparallelize which prevented them to be accelerated on GPUs In decomposable attentionapplied a selfattention mechanism tofeedforward networks which are easy to parallelize and achievedSOTAresult intextual entailmentwith an order of magnitude less parameters than LSTMsOne of its authors Jakob Uszkoreit suspected that attentionwithoutrecurrence is sufficient for language translation thus the title attention isallyou needThat hypothesis was against conventional wisdom of the time and even his father a wellknown computational linguist was skepticalIn the same year selfattention was proposed for LSTMs",
            "In the original encoderdecoder transformer model was proposed in the Attention is all you need paper At the time the focus of the research was on improvingseqseqformachine translation by removing its recurrence to process all tokens in parallel but preserving its dotproduct attention mechanism to keep its text processing performanceIts parallelizability was an important factor to its widespread use in large neural networks",
            "Already in spring even before the Attention is all you need preprint was published one of the coauthors applied the decoderonly variation of the architecture to generate fictitious Wikipedia articlesTransformer architecture is now used in manygenerative modelsthat contribute to the ongoingAI boom",
            "In language modellingELMo was a bidirectional LSTM that produces contextualizedword embeddings improving upon the line of research frombag of wordsandwordvec It was followed byBERT an encoderonly Transformer modelIn October Google started using BERT to process search queriesIn Google Translate replaced the previous RNNencoderRNNdecoder model by a TransformerencoderRNNdecoder model",
            "Starting in the OpenAIGPT seriesof decoderonly Transformers became state of the art innatural language generation In a chatbot based on GPTChatGPT became unexpectedly populartriggering a boom aroundlarge language models",
            "Since Transformers have been applied in modalities beyond text including thevision transformerspeech recognitionroboticsandmultimodalThe vision transformer in turn stimulated new developments inconvolutional neural networksImage and video generators likeDALLEStable Diffusion andSora are based on the Transformer architecture",
            "The plain transformer architecture had difficulty converging In the original paperthe authors recommended using learning rate warmup That is the learning rate should linearly scale up from to maximal value for the first part of the training before decaying again",
            "A paper found that usinglayer normalizationbefore multiheaded attention and feedforward layers stabilizes training not requiring learning rate warmup",
            "Transformers typically are first pretrained byselfsupervised learningon a large generic dataset followed bysupervisedfinetuningon a small taskspecific dataset The pretrain dataset is typically an unlabeled large corpus such asThe Pile Tasks for pretraining and finetuning commonly include",
            "TheT transformerreportdocuments a large number ofnatural languagepretraining tasks Some examples are",
            "Note that while each of these tasks is trivial or obvious for human native speakers of the language they have typically proved challenging for previous generations of machine learning architecture",
            "In general there are classes of language modelling tasks maskedautoregressiveand prefixLMThese classes are independent of a specific modeling architecture such as Transformer but they are often discussed in the context of Transformer",
            "In a masked taskone or more of the tokens is masked out and the model would produce a probability distribution predicting what the maskedout tokens are based on the context Theloss functionfor the task is typically sum oflogperplexitiesfor the maskedout tokensLosstmasked tokenslndisplaystyle textLosssum _tin textmasked tokenslnand the model is trained to minimize this loss function TheBERT series of modelsare trained for masked token prediction and another task",
            "In an autoregressive taskthe entire sequence is masked at first and the model produces a probability distribution for the first token Then the first token is revealed and the model predicts the second token and so on The loss function for the task is still typically the same TheGPT series of modelsare trained by autoregressive tasks",
            "In a prefixLM taskthe sequence is divided into two parts The first part is presented as context and the model predicts the first token of the second part Then that would be revealed and the model predicts the second token and so on The loss function for the task is still typically the same TheT series of modelsare trained by prefixLM tasks",
            "Note that masked as in masked language modelling is not masked as in masked attention and prefixLM is notprefixLM",
            "All transformers have the same primary components",
            "The following description follows exactly the Transformer as described in the original paper There are variants described in thefollowing section",
            "By convention we write all vectors as row vectors This for example means that pushing a vector through a linear layer means multiplying it by a weight matrix on the right asxWdisplaystyle xW",
            "As the Transformer architecture natively processes numerical data not text there must be a translation between text and tokens A token is an integer that represents a character or a short segment of characters On the input side the input text is parsed into a token sequence Similarly on the output side the output tokens are parsed back to text The module doing the conversion between texts and token sequences is atokenizer",
            "The set of all tokens is the vocabulary of the tokenizer and its size is thevocabulary sizenvocabularydisplaystyle n_textvocabulary When faced with tokens outside the vocabulary typically a special token is used written as for unknown",
            "Some commonly used tokenizers arebyte pair encoding WordPiece and SentencePiece",
            "Each token is converted into an embedding vector via alookup table Equivalently stated it multiplies aonehotrepresentation of the token by an embedding matrixMdisplaystyle M For example if the input token isdisplaystyle then the onehot representation isdisplaystyle and its embedding vector isEmbedMdisplaystyle mathrm Embed MThe token embedding vectors are added to their respective positional encoding vectors producing the sequence of input vectors",
            "The number of dimensions in an embedding vector is calledhidden sizeorembedding sizeand written asdembdisplaystyle d_textemb",
            "An unembedding layer is almost the reverse of an embedding layer Whereas an embedding layer converts a token into a vector an unembedding layer converts a vector into a probability distribution over tokens",
            "The unembedding layer is a linearsoftmaxlayerUnEmbedsoftmaxdisplaystyle mathrm UnEmbed mathrm softmax The matrix has shapedisplaystyle The embedding matrixMdisplaystyle Mand the unembedding matrixWdisplaystyle Ware sometimes required to be transposes of each other a practice called weight tying",
            "A positional encoding is a fixedsize vector representation of the relative positions of tokens within a sequence it provides the transformer model with information aboutwherethe words are in the input sequence Without positional encoding the model would be unable to process input sequence as more than abag of words as for example both man bites dog and dog bites man would be processed exactly the same way",
            "The positional encoding is defined as a function of typefRRddZddisplaystyle fmathbb R to mathbb R ddin mathbb Z d whereddisplaystyle dis a positive eveninteger The full positional encoding defined in the original paperiskfkcoskddisplaystyle _kf_kcosquad forall kin ldots dwhereθtrkrNddisplaystyle theta frac trkrNd",
            "HereNdisplaystyle Nis a free parameter that should be significantly larger than the biggestkdisplaystyle kthat would be input into the positional encoding function The original paper usesNdisplaystyle N",
            "The function is in a simpler form when written as a complex function of typefRCddisplaystyle fmathbb R to mathbb C dfkddisplaystyle fleft_kldots frac dwhererNddisplaystyle rNd",
            "The main reason for using this positional encoding function is that using it shifts are linear transformationsfdiagfdisplaystyle fmathrm diag fwhereΔtRdisplaystyle Delta tin mathbb R is the distance one wishes to shift This allows the transformer to take any encoded position and find the encoding of the position nstepsahead or nstepsbehind by a matrix multiplication",
            "By taking a linear sum any convolution can also be implemented as linear transformationsjcjffdisplaystyle sum _jc_jfleftrightffor any constantscjdisplaystyle c_j This allows the transformer to take any encoded position and find a linear sum of the encoded locations of its neighbors This sum of encoded positions when fed into the attention mechanism would create attention weights on its neighbors much like what happens in aconvolutional neural networklanguage model In the authors words we hypothesized it would allow the model to easily learn to attend by relative position",
            "In typical implementations all operations are done over the real numbers not the complex numbers but sincecomplex multiplication can be implemented as real by matrix multiplication this is a mere notational difference",
            "Like earlierseqseqmodels the original transformer model used anencoderdecoderarchitecture The encoder consists of encoding layers that process all the input tokens together one layer after another while the decoder consists of decoding layers that iteratively process the encoders output and the decoders output tokens so far",
            "The purpose of each encoder layer is to create contextualized representations of the tokens where each representation corresponds to a token that mixes information from other input tokens via selfattention mechanism Each decoder layer contains two attention sublayers crossattention for incorporating the output of encoder and selfattention for mixing information among the input tokens to the decoder",
            "Both the encoder and decoder layers have afeedforward neural networkfor additional processing of their outputs and contain residual connections and layer normalization stepsThese feedforward layers contain most of the parameters in a Transformer model",
            "The feedforward network modules in a Transformer are layeredmultilayer perceptronsFFNϕbWbdisplaystyle mathrm FFN phi bWbwhereϕdisplaystyle phi is its activation function The original Transformer usedReLUactivation",
            "The number of neurons in the middle layer is calledintermediate sizefilter sizeorfeedforward sizeIt is typically larger than the embedding size For example in both GPT series and BERT series the intermediate size of a model is times its embedding sizedffndembdisplaystyle d_textffnd_textemb",
            "The attention mechanism used in the Transformer architecture are scaleddotproductattentionunits For each unit the transformer model learns three weight matrices the query weightsWQdisplaystyle WQ the key weightsWKdisplaystyle WK and the value weightsWVdisplaystyle WV",
            "The module takes three sequences a query sequence a key sequence and a value sequence The query sequence is a sequence of lengthℓseq querydisplaystyle ell _textseq query and each entry is a vector of dimensiondemb querydisplaystyle d_textemb query Similarly for the key and value sequences",
            "For each vectorxiquerydisplaystyle x_itextqueryin the query sequence it is multiplied by a matrixWQdisplaystyle WQto produce a query vectorqixiqueryWQdisplaystyle q_ix_itextqueryWQ The matrix of all query vectors is the query matrixQXqueryWQdisplaystyle QX_textqueryWQSimilarly we construct the key matrixKXkeyWKdisplaystyle KX_textkeyWKand the value matrixVXvalueWVdisplaystyle VX_textvalueWV",
            "It is usually the case that allWQWKWVdisplaystyle WQWKWVare square matrices meaningdemb querydquerydisplaystyle d_textemb queryd_textquery etc",
            "Attention weights are calculated using the query and key vectors the attention weightaijdisplaystyle a_ijfrom tokenidisplaystyle ito tokenjdisplaystyle jis thedot productbetweenqidisplaystyle q_iandkjdisplaystyle k_j The attention weights are divided by the square root of the dimension of the key vectorsdkdisplaystyle sqrt d_k which stabilizes gradients during training and passed through asoftmaxwhich normalizes the weights The fact thatWQdisplaystyle WQandWKdisplaystyle WKare different matrices allows attention to be nonsymmetric if tokenidisplaystyle iattends to tokenjdisplaystyle j this does not necessarily mean that tokenjdisplaystyle jwill attend to tokenidisplaystyle i The output of the attention unit for tokenidisplaystyle iis the weighted sum of the value vectors of all tokens weighted byaijdisplaystyle a_ij the attention from tokenidisplaystyle ito each token",
            "The attention calculation for all tokens can be expressed as one large matrix calculation using thesoftmax function which is useful for training due to computational matrix operation optimizations that quickly compute matrix operations The matricesQdisplaystyle QKdisplaystyle KandVdisplaystyle Vare defined as the matrices where theidisplaystyle ith rows are vectorsqidisplaystyle q_ikidisplaystyle k_i andvidisplaystyle v_irespectively Then we can represent the attention asAttentionsoftmaxVdisplaystyle beginalignedtextAttentiontextsoftmaxleftVendaligned",
            "where the softmax is applied over each of the rows of the matrix",
            "The number of dimensions in a query vector isquery sizedquerydisplaystyle d_textqueryand similarly for thekey sizedkeydisplaystyle d_textkeyandvalue sizedvaluedisplaystyle d_textvalue The output dimension of an attention head is itshead dimensiondheaddisplaystyle d_texthead The attention mechanism requires the following three equalities to holdℓseq keyℓseq valuedquerydkeydvaluedheaddisplaystyle ell _textseq keyell _textseq valued_textqueryd_textkeyd_textvalued_textheadbut is otherwise unconstrained",
            "If the attention head is used in a selfattention fashion thenXqueryXkeyXvaluedisplaystyle X_textqueryX_textkeyX_textvalue If the attention head is used in a crossattention fashion then usuallyXqueryXkeyXvaluedisplaystyle X_textqueryneq X_textkeyX_textvalue It is theoretically possible for all three to be different but that is rarely the case in practice",
            "One set ofdisplaystyle leftmatrices is called anattention head and each layer in a transformer model has multiple attention heads While each attention head attends to the tokens that are relevant to each token multiple attention heads allow the model to do this for different definitions of relevance Specifically the query and key projection matricesWQdisplaystyle WQandWKdisplaystyle WK which are involved in the attention score computation defines the relevance Meanwhile the value projection matrixWVdisplaystyle WV in combination with the part of the output projection matrixWOdisplaystyle WO determines how the attended tokens influence what information is passed to subsequent layers and ultimately the output logits In addition the scope of attention or the range of token relationships captured by each attention head can expand as tokens pass through successive layers This allows the model to capture more complex and longrange dependencies in deeper layers Many transformer attention heads encode relevance relations that are meaningful to humans For example some attention heads can attend mostly to the next word while others mainly attend from verbs to their direct objectsThe computations for each attention head can be performed inparallel which allows for fast processing The outputs for the attention layer are concatenated to pass into thefeedforward neural networklayers",
            "Concretely let the multiple attention heads be indexed byidisplaystyle i then we haveMultiheadedAttentionConcatiWOdisplaystyle textMultiheadedAttentiontextConcat_iin WOwhere the matrixXdisplaystyle Xis the concatenation of word embeddings and the matricesWiQWiKWiVdisplaystyle W_iQW_iKW_iVare projection matrices owned by individual attention headidisplaystyle i andWOdisplaystyle WOis a final projection matrix owned by the whole multiheaded attention head",
            "It is theoretically possible for each attention head to have a different head dimensiondheaddisplaystyle d_texthead but that is rarely the case in practice",
            "As an example in the smallest GPT model there are only selfattention mechanisms It has the following dimensionsdembnheaddheaddisplaystyle d_textembn_textheadd_textheadSincedisplaystyle times its output projection matrixWORdisplaystyle WOin mathbb R times is a square matrix",
            "It may be necessary to cut out attention links between some wordpairs For example the decoder when decoding for the token positiontdisplaystyle t should not have access to the token at positiontdisplaystyle t This may be accomplished before the softmax stage by adding a mask matrixMdisplaystyle Mthat isdisplaystyle infty at entries where the attention link must be cut anddisplaystyle at other placesMaskedAttentionsoftmaxVdisplaystyle beginalignedtextMaskedAttentiontextsoftmaxleftVendalignedA nonmasked attention module can be thought of as a masked attention module where the mask has all entries zero",
            "For example the following matrix is commonly used in decoder selfattention modules called causal maskingMcausaldisplaystyle M_textcausalbeginbmatrixinfty infty dots infty infty dots infty dots infty vdots vdots vdots ddots vdots dots endbmatrixIn words it means that each token can pay attention to itself and every token before it but not any after it As an example of an uncommon use of mask matrix theXLNetconsiders all masks of the formPMcausalPdisplaystyle PM_textcausalP wherePdisplaystyle Pis a randompermutation matrix",
            "An encoder consists of an embedding layer followed by multiple encoder layers",
            "Each encoder layer consists of two major components a selfattention mechanism and a feedforward layer It takes an input as a sequence of input vectors applies the selfattention mechanism to produce an intermediate sequence of vectors then applies the feedforward layer for each vector individually Schematically we havegiven input vectorshhcombine them into a matrixHEncoderLayerdisplaystyle beginalignedtextgiven input vectors h_h_dots textcombine them into a matrix Hbeginbmatrixh_h_vdots endbmatrixtextEncoderLayerbeginbmatrixtextFFN_textFFN_vdots endbmatrixendaligned",
            "whereFFNdisplaystyle textFFNstands for feedforward network We can more succinctly write it asEncoderLayerFFNdisplaystyle textEncoderLayertextFFNwith the implicit convention that theFFNdisplaystyle textFFNis applied to each row of the matrix individually",
            "The encoder layers are stacked The first encoder layer takes the sequence of input vectors from the embedding layer producing a sequence of vectors This sequence of vectors is processed by the second encoder and so on The output from the final encoder layer is then used by the decoder",
            "As the encoder processes the entire input all at once every token can attend to every other token so there is no need for causal masking",
            "A decoder consists of an embedding layer followed by multiple decoder layers followed by an unembedding layer",
            "Each decoder consists of three major components a causally masked selfattention mechanism a crossattention mechanism and a feedforward neural network The decoder functions in a similar fashion to the encoder but an additional attention mechanism is inserted which instead draws relevant information from the encodings generated by the encoders This mechanism can also be called theencoderdecoder attention",
            "Like the first encoder the first decoder takes positional information and embeddings of the output sequence as its input rather than encodings The transformer must not use the current or future output to predict an output so the output sequence must be partially masked to prevent this reverse information flowThis allows forautoregressivetext generation For decoding alltoall attention is inappropriate because a token cannot attend to tokens not yet generated Thus the selfattention module in the decoder is causally masked",
            "In contrast the crossattention mechanism attends to the output vectors of the encoder which is computed before the decoder starts decoding Consequently there is no need for masking in the crossattention mechanism",
            "Schematically we haveHMaskedMultiheadedAttentionDecoderLayerFFNdisplaystyle beginalignedHtextMaskedMultiheadedAttentiontextDecoderLayertextFFNendalignedwhereHEdisplaystyle HEis the matrix with rows being the output vectors from the encoder",
            "The last decoder is followed by a final unembedding layer to produce the output probabilities over the vocabulary Then one of the tokens is sampled according to the probability and the decoder can be run again to produce the next token etc autoregressively generating output text",
            "Each encoder layer contains sublayers the selfattention and the feedforward network Each decoder layer contains sublayers the causally masked selfattention the crossattention and the feedforward network",
            "The final points of detail are theresidual connectionsandlayer normalization which while conceptually unnecessary are necessary for numerical stability and convergence Similarly to how the feedforward network modules are applied individually to each vector the LayerNorm is also applied individually to each vector",
            "There are two common conventions in use thepostLNand thepreLNconvention In the postLN convention the output of each sublayer isLayerNormdisplaystyle mathrm LayerNorm whereSublayerdisplaystyle mathrm Sublayer is the function implemented by the sublayer itself",
            "In the preLN convention the output of each sublayer isxSublayerdisplaystyle xmathrm Sublayer The original Transformer used the postLN convention It was difficult to train and required careful hyperparameter tuning and a warmup in learning rate where it starts small and gradually increases The preLN convention proposed several times in was found to be easier to train requiring no warmup leading to faster convergence",
            "The following is the pseudocode for a standard preLN encoderdecoder Transformer adapted from",
            "The Transformer architecture being modular allows variations Several common variations are described here",
            "An encoderonly Transformer applies the encoder to map an input text into a sequence of vectors that represent the input text This is usually used for text embedding andrepresentation learningfor downstream applicationsBERTis encoderonly They are less often used currently as they were found to be not significantly better than training an encoderdecoder Transformer then taking just the encoder",
            "A decoderonly Transformer is not literally decoderonly since without an encoder the crossattention mechanism has nothing to attend to Thus the decoder layers in a decoderonly Transformer is composed of just two sublayers the causally masked selfattention and the feedforward network This is usually used fortext generationandinstruction following The models in theGPT seriesandChinchilla seriesare decoderonly",
            "An encoderdecoder Transformer is generally the same as the original Transformer with sublayers per encoder layer and sublayers per decoder layer etc They might have minor architectural improvements such asalternative activation functionschanging the location of normalization etc This is also usually used fortext generationandinstruction following The models in theT seriesare encoderdecoder",
            "A prefixLM is a decoderonly architecture but with prefix masking which is different from causal masking Specifically it has mask of the form Figure MprefixLMdisplaystyle M_textprefixLMbeginbmatrixmathbf infty mathbf M_textcausalendbmatrixwhere the first columns correspond to the prefix and the subsequent columns correspond to the autoregressively generated text based on the prefix They resemble encoderdecoder models but has less sparsity Such models are rarely used though they are cited as theoretical possibilities and benchmarked comparisons",
            "There are also mixed seqseq models For example in Google Translate replaced the previous RNNencoderRNNdecoder model by a TransformerencoderRNNdecoder model on the argument that an RNNdecoder runs much faster than Transformerdecoder when run autoregressively",
            "The original transformer usesReLUactivation function Other activation functions were developed TheLlama seriesused SwiGLUboth GPT and BERTused GELU",
            "Alternative activation functions are often used in combination withGated Linear Unitsin the feedforward module",
            "The normalization used in the Transformer can be different from LayerNorm One example isRMSNormwhich is used in theLlama series Other examples include CapsuleNormScaleNormor FixNorm",
            "Transformers may use other positional encoding methods than sinusoidal",
            "The original Transformer paper reported using a learned positional encodingbut finding it not superior to the sinusoidal oneLaterfound that causal masking itself provides enough signal to a Transformer decoder that it can learn to implicitly perform absolute positional encoding without the positional encoding module",
            "RoPE is best explained by considering a list of dimensional vectorsdisplaystyle Now pick some angleθdisplaystyle theta Then RoPE encoding isRoPExmmxmcosmθxmsinmθxmcosmθxmsinmθdisplaystyle textRoPEbig x_mmbig beginpmatrixcos mtheta sin mtheta sin mtheta cos mtheta endpmatrixbeginpmatrixx_mx_mendpmatrixbeginpmatrixx_mcos mtheta x_msin mtheta x_mcos mtheta x_msin mtheta endpmatrixEquivalently if we write the dimensional vectors as complex numberszmxmixmdisplaystyle z_mx_mix_m then RoPE encoding is just multiplication by an angleRoPEeimθzmdisplaystyle textRoPEbig eimtheta z_mFor a list ofndisplaystyle ndimensional vectors a RoPE encoder is defined by a sequence of anglesθθdisplaystyle theta theta Then the RoPE encoding is applied to each pair of coordinates",
            "The benefit of RoPE is that the dotproduct between two vectors depends on their relative location onlyRoPETRoPERoPETRoPEdisplaystyle textRoPEbig TtextRoPEbig textRoPEbig TtextRoPEbig for any integerkdisplaystyle k",
            "ALiBi is not areplacementfor the positional encoder on the original transformer Instead it is anadditionalpositional encoder that is directly plugged into the attention mechanism Specifically the ALiBi attention mechanism isAttentionsoftmaxVdisplaystyle beginalignedtextAttentiontextsoftmaxleftVendalignedHeresdisplaystyle sis a real number andBdisplaystyle Bis thelinear biasmatrix defined byBdisplaystyle Bbeginpmatrixcdots cdots cdots cdots vdots vdots vdots vdots ddots endpmatrixin other wordsBijjidisplaystyle B_ijji The idea being that the linear bias matrix is a softened mask Just asdisplaystyle represent full attention paid anddisplaystyle infty represents no attention paid the linear bias matrix increases attention paid in one direction and decreases attention paid in the other direction",
            "ALiBi allows pretraining on short context windows then finetuning on longer context windows Since it is directly plugged into the attention mechanism it can be combined with any positional encoder that is plugged into the bottom of the entire network",
            "Relative Position Encodingsis similar to ALiBi but more genericAttentionsoftmaxVdisplaystyle beginalignedtextAttentiontextsoftmaxleftVendalignedwhereBdisplaystyle Bis aToeplitz matrix that isBijBijdisplaystyle B_ijB_ijwheneverijijdisplaystyle ijij This is contrasted with the original sinusoidal positional encoding which is an absolute positional encoding",
            "The transformer model has been implemented in standard deep learningframeworkssuch asTensorFlowandPyTorchTransformersis a library produced byHugging Facethat supplies transformerbased architectures and pretrained models",
            "FlashAttentionis an algorithm that implements the transformer attention mechanism efficiently on a GPU It performsmatrix multiplications in blocks such that each block fits within thecacheof a GPU and by careful management of the blocks it minimizes data copying between GPU caches",
            "An improved version FlashAttentionwas developed to cater to the rising demand for language models capable of handling longer context lengths It offers enhancements in work partitioning and parallelism enabling it to achieve up to TFLOPss onAGPUs a x speed increase over the original FlashAttention",
            "Key advancements in FlashAttention include the reduction of nonmatmul FLOPs improved parallelism over the sequence length dimension better work partitioning between GPU warps and added support for head dimensions up to and multiquery attention and groupedquery attention",
            "Benchmarks revealed FlashAttention to be up to x faster than FlashAttention and up to x faster than a standard attention implementation in PyTorch Future developments include optimization for new hardware likeHGPUs and new data types like FP",
            "MultiQuery Attention changes the multiheaded attention mechanismWhereas normally",
            "MultiheadedAttentionConcatiWOdisplaystyle textMultiheadedAttentiontextConcat_iin leftrightWOwith MultiQuery Attention there is just oneWKWVdisplaystyle WKWV thus",
            "MultiQueryAttentionConcatiWOdisplaystyle textMultiQueryAttentiontextConcat_iin leftrightWO",
            "This has a neutral effect on model quality and training speed but increases inference speed",
            "More generally groupedquery attention partitions attention heads into groups each of which shares the keyvalue pair MQA is GQA with one group while standard multiheaded atteniton is GQA with the maximal number of groups",
            "When an autoregressive transformer is used for inference such as generating text the query vector is different at each step but the alreadycomputed key and value vectors are always the same TheKV cachingmethod saves the computed key and value vectors at each attention block so that they are not recomputed at each new tokenPagedAttentionappliesmemory pagingto KV caching",
            "If a transformer is used with a bakedin prompt such as then the key and value vectors can be computed for the prompt and saved on disk The saving in compute is significant when the model is used for many short interactions such as in online chatbots",
            "Transformers are used in large language models for autoregressive sequence generation generating a stream of text one token at a time However in most settings decoding from language models is memorybound meaning that we have spare compute power available Speculative decodinguses this spare compute power by computing several tokens in parallel Similarly tospeculative executionin CPUs future tokens are computed concurrently by speculating on the value of previous tokens and are later discarded if it turns out the speculation was incorrect",
            "Specifically consider a transformer model like GPT with a context window size of To generate an entire context window autoregressively with greedy decoding it must be run for times each time generating a tokenxxxdisplaystyle x_x_x_ However if we had some educated guess for the values of these tokens we could verify all of them in parallel in one run of the model by checking that eachxtdisplaystyle x_tis indeed the token with the largest loglikelihood in thetdisplaystyle tth output",
            "In speculative decoding a smaller model or some other simple heuristic is used to generate a few speculative tokens that are subsequently verified by the larger model For example suppose a small model generated four speculative tokensxxxxdisplaystyle tilde x_tilde x_tilde x_tilde x_ These tokens are run through the larger model and onlyxdisplaystyle tilde x_andxdisplaystyle tilde x_are accepted The same run of the large model already generated a new tokenxdisplaystyle x_to replacexdisplaystyle tilde x_ andxdisplaystyle tilde x_is completely discarded The process then repeats until all tokens are generated",
            "For nongreedy decoding similar ideas apply except the speculative tokens are accepted or rejected stochastically in a way that guarantees the final output distribution is the same as if speculative decoding was not used",
            "Training transformerbased architectures can be expensive especially for long inputsMany methods have been developed to attempt to address the issueLong Range Arenais a standard benchmark for comparing the behavior of transformer architectures over long inputs",
            "The standard attention graph is either alltoall or causal both of which scales asOdisplaystyle OwhereNdisplaystyle Nis the number of tokens in a sequence",
            "Reformer reduces the computational load fromOdisplaystyle OtoOdisplaystyle Oby usinglocalitysensitive hashingand reversible layers",
            "Sparse attentionuses attention graphs that grows slower thanOdisplaystyle O For example BigBird uses randomsmallworld networkswhich grows asOdisplaystyle O",
            "Ordinary transformers require a memory size that is quadratic in the size of the context window Attentionfree transformersreduce this to a linear dependence while still retaining the advantages of a transformer by linking the key to the value",
            "Random Feature Attention usesFourier random featuresφDTdisplaystyle varphi frac sqrt DTwherewwDdisplaystyle w_w_Dare independent samples from the normal distributionNdisplaystyle N This choice of parameters satisfyEexyσdisplaystyle mathbb E efrac xysigma orexyσEexσφeyσφdisplaystyle elangle xyrangle sigma mathbb E approx langle exsigma varphi eysigma varphi rangle Consequently the oneheaded attention with one query can be written asAttentionsoftmaxVφTiekiσφviTφTiekiσφdisplaystyle textAttentiontextsoftmaxleftVapprox frac varphi Tsum _iek_isigma varphi v_iTvarphi Tsum _iek_isigma varphi whereσdKdisplaystyle sigma d_K Similarly for multiple queries and for multiheaded attention",
            "This approximation can be computed in linear time as we can compute the matrixφviTdisplaystyle varphi v_iTfirst then multiply it with the query In essence we have managed to obtain a more precise version ofAttentionsoftmaxVQdisplaystyle textAttentiontextsoftmaxleftVapprox QPerformer uses the same Random Feature Attention butwwDdisplaystyle w_w_Dare first independently sampled from the normal distributionNdisplaystyle N then they areGramSchmidt processed",
            "Transformers can also be usedadapted for modalities beyond just text usually by finding a way to tokenize the modality",
            "Multimodal models can either be trained from scratch or by finetuning A study found that Transformers pretrained only on natural language can be finetuned on only of parameters and become competitive with LSTMs on a variety of logical and visual tasks demonstratingtransfer learningThe LLaVA was a visionlanguage model composed of a language model and a vision model connected by a linear layer Only the linear layer is finetuned",
            "Vision transformersadapt the transformer to computer vision by breaking down input images as a series of patches turning them into vectors and treating them like tokens in a standard transformer",
            "Conformerand laterWhisperfollow the same pattern forspeech recognition first turning the speech signal into aspectrogram which is then treated like an image ie broken down into a series of patches turned into vectors and treated like tokens in a standard transformer",
            "Perceiversare a variant of Transformers designed for multimodality",
            "For image generation notable architectures areDALLE Parti Phenaki and Muse Unlike later models DALLE is not a diffusion model Instead it uses a decoderonly Transformer that autoregressively generates a text followed by the token representation of an image which is then converted by avariational autoencoderto an imageParti is an encoderdecoder Transformer where the encoder processes a text prompt and the decoder generates a token representation of an imageMuse is an encoderonly Transformer that is trained to predict masked image tokens from unmasked image tokens During generation all input tokens are masked and the highestconfidence predictions are included for the next iteration until all tokens are predictedPhenaki is a texttovideo model It is a bidirectional masked transformer conditioned on precomputed text tokens The generated tokens are then decoded to a video",
            "The transformer has had great success innatural language processing Manylarge language modelssuch asGPTGPTGPTAlbertAGPTClaudeBERTXLNetRoBERTaandChatGPTdemonstrate the ability of transformers to perform a wide variety of NLPrelated subtasks and their related realworld or practical applications including",
            "Beyond traditional NLP the transformer architecture has had success in other applications such as"
        ]
    },
    "extracted_links": [
        {
            "link": "https://en.wikipedia.org#bodyContent",
            "topic": "Jump to content"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Main_Page",
            "topic": "Main page"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Wikipedia:Contents",
            "topic": "Contents"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Portal:Current_events",
            "topic": "Current events"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Special:Random",
            "topic": "Random article"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Wikipedia:About",
            "topic": "About Wikipedia"
        },
        {
            "link": "https://en.wikipedia.org//en.wikipedia.org/wiki/Wikipedia:Contact_us",
            "topic": "Contact us"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Help:Contents",
            "topic": "Help"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Help:Introduction",
            "topic": "Learn to edit"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Wikipedia:Community_portal",
            "topic": "Community portal"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Special:RecentChanges",
            "topic": "Recent changes"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Wikipedia:File_upload_wizard",
            "topic": "Upload file"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Special:Search",
            "topic": "Search"
        },
        {
            "link": "https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en",
            "topic": "Donate"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Special:CreateAccount&returnto=Transformer+%28deep+learning+architecture%29",
            "topic": "Create account"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Special:UserLogin&returnto=Transformer+%28deep+learning+architecture%29",
            "topic": "Log in"
        },
        {
            "link": "https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en",
            "topic": "Donate"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Special:CreateAccount&returnto=Transformer+%28deep+learning+architecture%29",
            "topic": "Create account"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Special:UserLogin&returnto=Transformer+%28deep+learning+architecture%29",
            "topic": "Log in"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Help:Introduction",
            "topic": "learn more"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Special:MyContributions",
            "topic": "Contributions"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Special:MyTalk",
            "topic": "Talk"
        },
        {
            "link": "https://en.wikipedia.org#",
            "topic": "(Top)"
        },
        {
            "link": "https://en.wikipedia.org#History",
            "topic": "History"
        },
        {
            "link": "https://en.wikipedia.org#Predecessors",
            "topic": "Predecessors"
        },
        {
            "link": "https://en.wikipedia.org#Attention_with_seq2seq",
            "topic": "Attention with seq2seq"
        },
        {
            "link": "https://en.wikipedia.org#Parallelizing_attention",
            "topic": "Parallelizing attention"
        },
        {
            "link": "https://en.wikipedia.org#AI_boom_era",
            "topic": "AI boom era"
        },
        {
            "link": "https://en.wikipedia.org#Training",
            "topic": "Training"
        },
        {
            "link": "https://en.wikipedia.org#Methods_for_stabilizing_training",
            "topic": "Methods for stabilizing training"
        },
        {
            "link": "https://en.wikipedia.org#Pretrain-finetune",
            "topic": "Pretrain-finetune"
        },
        {
            "link": "https://en.wikipedia.org#Tasks",
            "topic": "Tasks"
        },
        {
            "link": "https://en.wikipedia.org#Architecture",
            "topic": "Architecture"
        },
        {
            "link": "https://en.wikipedia.org#Tokenization",
            "topic": "Tokenization"
        },
        {
            "link": "https://en.wikipedia.org#Embedding",
            "topic": "Embedding"
        },
        {
            "link": "https://en.wikipedia.org#Un-embedding",
            "topic": "Un-embedding"
        },
        {
            "link": "https://en.wikipedia.org#Positional_encoding",
            "topic": "Positional encoding"
        },
        {
            "link": "https://en.wikipedia.org#Encoder-decoder_(overview)",
            "topic": "Encoder-decoder (overview)"
        },
        {
            "link": "https://en.wikipedia.org#Feedforward_network",
            "topic": "Feedforward network"
        },
        {
            "link": "https://en.wikipedia.org#Scaled_dot-product_attention",
            "topic": "Scaled dot-product attention"
        },
        {
            "link": "https://en.wikipedia.org#Attention_head",
            "topic": "Attention head"
        },
        {
            "link": "https://en.wikipedia.org#Multiheaded_attention",
            "topic": "Multiheaded attention"
        },
        {
            "link": "https://en.wikipedia.org#Masked_attention",
            "topic": "Masked attention"
        },
        {
            "link": "https://en.wikipedia.org#Encoder",
            "topic": "Encoder"
        },
        {
            "link": "https://en.wikipedia.org#Decoder",
            "topic": "Decoder"
        },
        {
            "link": "https://en.wikipedia.org#Full_transformer_architecture",
            "topic": "Full transformer architecture"
        },
        {
            "link": "https://en.wikipedia.org#Sublayers",
            "topic": "Sublayers"
        },
        {
            "link": "https://en.wikipedia.org#Pseudocode",
            "topic": "Pseudocode"
        },
        {
            "link": "https://en.wikipedia.org#Terminology",
            "topic": "Terminology"
        },
        {
            "link": "https://en.wikipedia.org#Subsequent_work",
            "topic": "Subsequent work"
        },
        {
            "link": "https://en.wikipedia.org#Alternative_activation_functions",
            "topic": "Alternative activation functions"
        },
        {
            "link": "https://en.wikipedia.org#Alternative_normalizations",
            "topic": "Alternative normalizations"
        },
        {
            "link": "https://en.wikipedia.org#Alternative_positional_encodings",
            "topic": "Alternative positional encodings"
        },
        {
            "link": "https://en.wikipedia.org#RoPE",
            "topic": "RoPE"
        },
        {
            "link": "https://en.wikipedia.org#ALiBi",
            "topic": "ALiBi"
        },
        {
            "link": "https://en.wikipedia.org#Relative_Position_Encodings",
            "topic": "Relative Position Encodings"
        },
        {
            "link": "https://en.wikipedia.org#Efficient_implementation",
            "topic": "Efficient implementation"
        },
        {
            "link": "https://en.wikipedia.org#FlashAttention",
            "topic": "FlashAttention"
        },
        {
            "link": "https://en.wikipedia.org#Multi-Query_Attention",
            "topic": "Multi-Query Attention"
        },
        {
            "link": "https://en.wikipedia.org#Caching",
            "topic": "Caching"
        },
        {
            "link": "https://en.wikipedia.org#Speculative_decoding",
            "topic": "Speculative decoding"
        },
        {
            "link": "https://en.wikipedia.org#Sub-quadratic_transformers",
            "topic": "Sub-quadratic transformers"
        },
        {
            "link": "https://en.wikipedia.org#Alternative_attention_graphs",
            "topic": "Alternative attention graphs"
        },
        {
            "link": "https://en.wikipedia.org#Random_Feature_Attention",
            "topic": "Random Feature Attention"
        },
        {
            "link": "https://en.wikipedia.org#Multimodality",
            "topic": "Multimodality"
        },
        {
            "link": "https://en.wikipedia.org#Applications",
            "topic": "Applications"
        },
        {
            "link": "https://en.wikipedia.org#See_also",
            "topic": "See also"
        },
        {
            "link": "https://en.wikipedia.org#Notes",
            "topic": "Notes"
        },
        {
            "link": "https://en.wikipedia.org#References",
            "topic": "References"
        },
        {
            "link": "https://en.wikipedia.org#Further_reading",
            "topic": "Further reading"
        },
        {
            "link": "https://ar.wikipedia.org/wiki/%D9%85%D8%AD%D9%88%D9%84_(%D8%AA%D8%B9%D9%84%D9%85_%D8%A7%D9%84%D8%A2%D9%84%D8%A9)",
            "topic": "العربية"
        },
        {
            "link": "https://ca.wikipedia.org/wiki/Transformador_(model_d%27aprenentatge_autom%C3%A0tic)",
            "topic": "Català"
        },
        {
            "link": "https://cs.wikipedia.org/wiki/Transform%C3%A1tor_(model_strojov%C3%A9ho_u%C4%8Den%C3%AD)",
            "topic": "Čeština"
        },
        {
            "link": "https://de.wikipedia.org/wiki/Transformer_(Maschinelles_Lernen)",
            "topic": "Deutsch"
        },
        {
            "link": "https://et.wikipedia.org/wiki/Transformer_(masin%C3%B5pe)",
            "topic": "Eesti"
        },
        {
            "link": "https://es.wikipedia.org/wiki/Transformador_(modelo_de_aprendizaje_autom%C3%A1tico)",
            "topic": "Español"
        },
        {
            "link": "https://eu.wikipedia.org/wiki/Transformer_(ikasketa_automatikoko_eredua)",
            "topic": "Euskara"
        },
        {
            "link": "https://fa.wikipedia.org/wiki/%D8%AA%D8%B1%D9%86%D8%B3%D9%81%D9%88%D8%B1%D9%85%D8%B1_(%DB%8C%D8%A7%D8%AF%DA%AF%DB%8C%D8%B1%DB%8C_%D8%B9%D9%85%DB%8C%D9%82)",
            "topic": "فارسی"
        },
        {
            "link": "https://fr.wikipedia.org/wiki/Transformeur",
            "topic": "Français"
        },
        {
            "link": "https://ga.wikipedia.org/wiki/Trasfhoirmeoir_(ailtireacht_domhainfhoghlama)",
            "topic": "Gaeilge"
        },
        {
            "link": "https://gl.wikipedia.org/wiki/Transformador_(modelo_de_aprendizaxe_autom%C3%A1tica)",
            "topic": "Galego"
        },
        {
            "link": "https://ko.wikipedia.org/wiki/%ED%8A%B8%EB%9E%9C%EC%8A%A4%ED%8F%AC%EB%A8%B8_(%EA%B8%B0%EA%B3%84_%ED%95%99%EC%8A%B5)",
            "topic": "한국어"
        },
        {
            "link": "https://hy.wikipedia.org/wiki/%D5%8F%D6%80%D5%A1%D5%B6%D5%BD%D6%86%D5%B8%D6%80%D5%B4%D5%A5%D6%80_(%D5%AD%D5%B8%D6%80%D5%A8_%D5%B8%D6%82%D5%BD%D5%B8%D6%82%D6%81%D5%B8%D6%82%D5%B4)",
            "topic": "Հայերեն"
        },
        {
            "link": "https://it.wikipedia.org/wiki/Trasformatore_(informatica)",
            "topic": "Italiano"
        },
        {
            "link": "https://he.wikipedia.org/wiki/%D7%98%D7%A8%D7%A0%D7%A1%D7%A4%D7%95%D7%A8%D7%9E%D7%A8_(%D7%9C%D7%9E%D7%99%D7%93%D7%AA_%D7%9E%D7%9B%D7%95%D7%A0%D7%94)",
            "topic": "עברית"
        },
        {
            "link": "https://ja.wikipedia.org/wiki/Transformer_(%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%83%A2%E3%83%87%E3%83%AB)",
            "topic": "日本語"
        },
        {
            "link": "https://pl.wikipedia.org/wiki/Transformator_(sztuczna_inteligencja)",
            "topic": "Polski"
        },
        {
            "link": "https://kaa.wikipedia.org/wiki/Transformator_(tere%C5%84_oq%C4%B1t%C4%B1w_arxitekturas%C4%B1)",
            "topic": "Qaraqalpaqsha"
        },
        {
            "link": "https://ru.wikipedia.org/wiki/%D0%A2%D1%80%D0%B0%D0%BD%D1%81%D1%84%D0%BE%D1%80%D0%BC%D0%B5%D1%80_(%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C_%D0%BC%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B3%D0%BE_%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D1%8F)",
            "topic": "Русский"
        },
        {
            "link": "https://simple.wikipedia.org/wiki/Transformer_(machine_learning_model)",
            "topic": "Simple English"
        },
        {
            "link": "https://ckb.wikipedia.org/wiki/%D8%AA%D8%B1%D8%A7%D9%86%D8%B3%D9%81%DB%86%D8%B1%D9%85%DB%95%D8%B1_(%D9%85%DB%86%D8%AF%DB%8E%D9%84%DB%8C_%D9%81%DB%8E%D8%B1%D8%A8%D9%88%D9%88%D9%86%DB%8C_%D8%A6%D8%A7%D9%85%DB%8E%D8%B1)",
            "topic": "کوردی"
        },
        {
            "link": "https://sr.wikipedia.org/wiki/Transformator_(model_ma%C5%A1inskog_u%C4%8Denja)",
            "topic": "Српски / srpski"
        },
        {
            "link": "https://sv.wikipedia.org/wiki/Transformator_(maskininl%C3%A4rningsmodell)",
            "topic": "Svenska"
        },
        {
            "link": "https://th.wikipedia.org/wiki/%E0%B8%97%E0%B8%A3%E0%B8%B2%E0%B8%99%E0%B8%AA%E0%B9%8C%E0%B8%9F%E0%B8%AD%E0%B8%A3%E0%B9%8C%E0%B9%80%E0%B8%A1%E0%B8%AD%E0%B8%A3%E0%B9%8C",
            "topic": "ไทย"
        },
        {
            "link": "https://uk.wikipedia.org/wiki/%D0%A2%D1%80%D0%B0%D0%BD%D1%81%D1%84%D0%BE%D1%80%D0%BC%D0%B5%D1%80_(%D0%B0%D1%80%D1%85%D1%96%D1%82%D0%B5%D0%BA%D1%82%D1%83%D1%80%D0%B0_%D0%B3%D0%BB%D0%B8%D0%B1%D0%BE%D0%BA%D0%BE%D0%B3%D0%BE_%D0%BD%D0%B0%D0%B2%D1%87%D0%B0%D0%BD%D0%BD%D1%8F)",
            "topic": "Українська"
        },
        {
            "link": "https://vi.wikipedia.org/wiki/Transformer_(m%C3%B4_h%C3%ACnh_h%E1%BB%8Dc_m%C3%A1y)",
            "topic": "Tiếng Việt"
        },
        {
            "link": "https://zh-yue.wikipedia.org/wiki/Transformer_(%E6%A9%9F%E6%A2%B0%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B)",
            "topic": "粵語"
        },
        {
            "link": "https://zh.wikipedia.org/wiki/Transformer%E6%A8%A1%E5%9E%8B",
            "topic": "中文"
        },
        {
            "link": "https://www.wikidata.org/wiki/Special:EntityPage/Q85810444#sitelinks-wikipedia",
            "topic": "Edit links"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)",
            "topic": "Article"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Talk:Transformer_(deep_learning_architecture)",
            "topic": "Talk"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)",
            "topic": "Read"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit",
            "topic": "Edit"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=history",
            "topic": "View history"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)",
            "topic": "Read"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit",
            "topic": "Edit"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=history",
            "topic": "View history"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Special:WhatLinksHere/Transformer_(deep_learning_architecture)",
            "topic": "What links here"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Special:RecentChangesLinked/Transformer_(deep_learning_architecture)",
            "topic": "Related changes"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Wikipedia:File_Upload_Wizard",
            "topic": "Upload file"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Special:SpecialPages",
            "topic": "Special pages"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&oldid=1260413106",
            "topic": "Permanent link"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=info",
            "topic": "Page information"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Special:CiteThisPage&page=Transformer_%28deep_learning_architecture%29&id=1260413106&wpFormIdentifier=titleform",
            "topic": "Cite this page"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Special:UrlShortener&url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FTransformer_%28deep_learning_architecture%29",
            "topic": "Get shortened URL"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Special:QrCode&url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FTransformer_%28deep_learning_architecture%29",
            "topic": "Download QR code"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Special:DownloadAsPdf&page=Transformer_%28deep_learning_architecture%29&action=show-download-screen",
            "topic": "Download as PDF"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&printable=yes",
            "topic": "Printable version"
        },
        {
            "link": "https://www.wikidata.org/wiki/Special:EntityPage/Q85810444",
            "topic": "Wikidata item"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Machine_learning",
            "topic": "Machine learning"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Data_mining",
            "topic": "data mining"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Supervised_learning",
            "topic": "Supervised learning"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Unsupervised_learning",
            "topic": "Unsupervised learning"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Semi-supervised_learning",
            "topic": "Semi-supervised learning"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Self-supervised_learning",
            "topic": "Self-supervised learning"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Reinforcement_learning",
            "topic": "Reinforcement learning"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Meta-learning_(computer_science)",
            "topic": "Meta-learning"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Online_machine_learning",
            "topic": "Online learning"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Batch_learning",
            "topic": "Batch learning"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Curriculum_learning",
            "topic": "Curriculum learning"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Rule-based_machine_learning",
            "topic": "Rule-based learning"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Neuro-symbolic_AI",
            "topic": "Neuro-symbolic AI"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Neuromorphic_engineering",
            "topic": "Neuromorphic engineering"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Quantum_machine_learning",
            "topic": "Quantum machine learning"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Statistical_classification",
            "topic": "Classification"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Generative_model",
            "topic": "Generative modeling"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Regression_analysis",
            "topic": "Regression"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Cluster_analysis",
            "topic": "Clustering"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Dimensionality_reduction",
            "topic": "Dimensionality reduction"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Density_estimation",
            "topic": "Density estimation"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Anomaly_detection",
            "topic": "Anomaly detection"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Data_cleaning",
            "topic": "Data cleaning"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Automated_machine_learning",
            "topic": "AutoML"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Association_rule_learning",
            "topic": "Association rules"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Semantic_analysis_(machine_learning)",
            "topic": "Semantic analysis"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Structured_prediction",
            "topic": "Structured prediction"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Feature_engineering",
            "topic": "Feature engineering"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Feature_learning",
            "topic": "Feature learning"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Learning_to_rank",
            "topic": "Learning to rank"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Grammar_induction",
            "topic": "Grammar induction"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Ontology_learning",
            "topic": "Ontology learning"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Multimodal_learning",
            "topic": "Multimodal learning"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Supervised_learning",
            "topic": "Supervised learning"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Statistical_classification",
            "topic": "classification"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Regression_analysis",
            "topic": "regression"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Apprenticeship_learning",
            "topic": "Apprenticeship learning"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Decision_tree_learning",
            "topic": "Decision trees"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Ensemble_learning",
            "topic": "Ensembles"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Bootstrap_aggregating",
            "topic": "Bagging"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Boosting_(machine_learning)",
            "topic": "Boosting"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Random_forest",
            "topic": "Random forest"
        },
        {
            "link": "https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm",
            "topic": "k-NN"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Linear_regression",
            "topic": "Linear regression"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Naive_Bayes_classifier",
            "topic": "Naive Bayes"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Artificial_neural_network",
            "topic": "Artificial neural networks"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Logistic_regression",
            "topic": "Logistic regression"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Perceptron",
            "topic": "Perceptron"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Relevance_vector_machine",
            "topic": "Relevance vector machine (RVM)"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Support_vector_machine",
            "topic": "Support vector machine (SVM)"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Cluster_analysis",
            "topic": "Clustering"
        },
        {
            "link": "https://en.wikipedia.org/wiki/BIRCH",
            "topic": "BIRCH"
        },
        {
            "link": "https://en.wikipedia.org/wiki/CURE_algorithm",
            "topic": "CURE"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Hierarchical_clustering",
            "topic": "Hierarchical"
        },
        {
            "link": "https://en.wikipedia.org/wiki/K-means_clustering",
            "topic": "k-means"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Fuzzy_clustering",
            "topic": "Fuzzy"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm",
            "topic": "Expectation–maximization (EM)"
        },
        {
            "link": "https://en.wikipedia.org/wiki/DBSCAN",
            "topic": "DBSCAN"
        },
        {
            "link": "https://en.wikipedia.org/wiki/OPTICS_algorithm",
            "topic": "OPTICS"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Mean_shift",
            "topic": "Mean shift"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Dimensionality_reduction",
            "topic": "Dimensionality reduction"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Factor_analysis",
            "topic": "Factor analysis"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Canonical_correlation",
            "topic": "CCA"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Independent_component_analysis",
            "topic": "ICA"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Linear_discriminant_analysis",
            "topic": "LDA"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Non-negative_matrix_factorization",
            "topic": "NMF"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Principal_component_analysis",
            "topic": "PCA"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Proper_generalized_decomposition",
            "topic": "PGD"
        },
        {
            "link": "https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding",
            "topic": "t-SNE"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Sparse_dictionary_learning",
            "topic": "SDL"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Structured_prediction",
            "topic": "Structured prediction"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Graphical_model",
            "topic": "Graphical models"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Bayesian_network",
            "topic": "Bayes net"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Conditional_random_field",
            "topic": "Conditional random field"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Hidden_Markov_model",
            "topic": "Hidden Markov"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Anomaly_detection",
            "topic": "Anomaly detection"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Random_sample_consensus",
            "topic": "RANSAC"
        },
        {
            "link": "https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm",
            "topic": "k-NN"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Local_outlier_factor",
            "topic": "Local outlier factor"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Isolation_forest",
            "topic": "Isolation forest"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Artificial_neural_network",
            "topic": "Artificial neural network"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Autoencoder",
            "topic": "Autoencoder"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Deep_learning",
            "topic": "Deep learning"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Feedforward_neural_network",
            "topic": "Feedforward neural network"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Recurrent_neural_network",
            "topic": "Recurrent neural network"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Long_short-term_memory",
            "topic": "LSTM"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Gated_recurrent_unit",
            "topic": "GRU"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Echo_state_network",
            "topic": "ESN"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Reservoir_computing",
            "topic": "reservoir computing"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Boltzmann_machine",
            "topic": "Boltzmann machine"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine",
            "topic": "Restricted"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Generative_adversarial_network",
            "topic": "GAN"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Diffusion_model",
            "topic": "Diffusion model"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Self-organizing_map",
            "topic": "SOM"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Convolutional_neural_network",
            "topic": "Convolutional neural network"
        },
        {
            "link": "https://en.wikipedia.org/wiki/U-Net",
            "topic": "U-Net"
        },
        {
            "link": "https://en.wikipedia.org/wiki/LeNet",
            "topic": "LeNet"
        },
        {
            "link": "https://en.wikipedia.org/wiki/AlexNet",
            "topic": "AlexNet"
        },
        {
            "link": "https://en.wikipedia.org/wiki/DeepDream",
            "topic": "DeepDream"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Neural_radiance_field",
            "topic": "Neural radiance field"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)",
            "topic": "Transformer"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Vision_transformer",
            "topic": "Vision"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Mamba_(deep_learning_architecture)",
            "topic": "Mamba"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Spiking_neural_network",
            "topic": "Spiking neural network"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Memtransistor",
            "topic": "Memtransistor"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Electrochemical_RAM",
            "topic": "Electrochemical RAM"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Reinforcement_learning",
            "topic": "Reinforcement learning"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Q-learning",
            "topic": "Q-learning"
        },
        {
            "link": "https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action",
            "topic": "SARSA"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Temporal_difference_learning",
            "topic": "Temporal difference (TD)"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Multi-agent_reinforcement_learning",
            "topic": "Multi-agent"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Self-play_(reinforcement_learning_technique)",
            "topic": "Self-play"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Active_learning_(machine_learning)",
            "topic": "Active learning"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Crowdsourcing",
            "topic": "Crowdsourcing"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Human-in-the-loop",
            "topic": "Human-in-the-loop"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback",
            "topic": "RLHF"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Coefficient_of_determination",
            "topic": "Coefficient of determination"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Confusion_matrix",
            "topic": "Confusion matrix"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Learning_curve_(machine_learning)",
            "topic": "Learning curve"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Receiver_operating_characteristic",
            "topic": "ROC curve"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Kernel_machines",
            "topic": "Kernel machines"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff",
            "topic": "Bias–variance tradeoff"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Computational_learning_theory",
            "topic": "Computational learning theory"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Empirical_risk_minimization",
            "topic": "Empirical risk minimization"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Occam_learning",
            "topic": "Occam learning"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Probably_approximately_correct_learning",
            "topic": "PAC learning"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Statistical_learning_theory",
            "topic": "Statistical learning"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory",
            "topic": "VC theory"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ECML_PKDD",
            "topic": "ECML PKDD"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems",
            "topic": "NeurIPS"
        },
        {
            "link": "https://en.wikipedia.org/wiki/International_Conference_on_Machine_Learning",
            "topic": "ICML"
        },
        {
            "link": "https://en.wikipedia.org/wiki/International_Conference_on_Learning_Representations",
            "topic": "ICLR"
        },
        {
            "link": "https://en.wikipedia.org/wiki/International_Joint_Conference_on_Artificial_Intelligence",
            "topic": "IJCAI"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Machine_Learning_(journal)",
            "topic": "ML"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Journal_of_Machine_Learning_Research",
            "topic": "JMLR"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence",
            "topic": "Glossary of artificial intelligence"
        },
        {
            "link": "https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research",
            "topic": "List of datasets for machine-learning research"
        },
        {
            "link": "https://en.wikipedia.org/wiki/List_of_datasets_in_computer_vision_and_image_processing",
            "topic": "List of datasets in computer vision and image processing"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Outline_of_machine_learning",
            "topic": "Outline of machine learning"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Template:Machine_learning",
            "topic": "v"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Template_talk:Machine_learning",
            "topic": "t"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Special:EditPage/Template:Machine_learning",
            "topic": "e"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Deep_learning",
            "topic": "deep learning"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Google",
            "topic": "Google"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Attention_(machine_learning)",
            "topic": "attention"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Attention_Is_All_You_Need",
            "topic": "Attention Is All You Need"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-2017_Attention_Is_All_You_Need-1",
            "topic": "[1]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Large_language_model#Tokenization",
            "topic": "tokens"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Word_embedding",
            "topic": "word embedding"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-2017_Attention_Is_All_You_Need-1",
            "topic": "[1]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Contextualization_(computer_science)",
            "topic": "contextualized"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Recurrent_neural_network",
            "topic": "recurrent neural architectures"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Long_short-term_memory",
            "topic": "long short-term memory"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-lstm1997-2",
            "topic": "[2]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Large_language_model",
            "topic": "large language models"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Wikipedia",
            "topic": "Wikipedia"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Text_corpus",
            "topic": "corpus"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Common_Crawl",
            "topic": "Common Crawl"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:7-3",
            "topic": "[3]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Machine_translation",
            "topic": "machine translation"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-inventors-4",
            "topic": "[4]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-inventconfirm-5",
            "topic": "[5]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Natural_language_processing",
            "topic": "natural language processing"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Computer_vision",
            "topic": "computer vision"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Vision_transformer",
            "topic": "vision transformers"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Reinforcement_learning",
            "topic": "reinforcement learning"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:10-6",
            "topic": "[6]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-7",
            "topic": "[7]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Audio_signal_processing",
            "topic": "audio"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-Robust_Speech_Recognition_via_Large-Scale_Weak_Supervision-8",
            "topic": "[8]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Multimodal_learning",
            "topic": "multimodal learning"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Robotics",
            "topic": "robotics"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-9",
            "topic": "[9]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Computer_chess",
            "topic": "chess"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-grandmaster-10",
            "topic": "[10]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Transfer_learning",
            "topic": "pre-trained systems"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Generative_pre-trained_transformer",
            "topic": "generative pre-trained transformers"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-wolf2020-11",
            "topic": "[11]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/BERT_(language_model)",
            "topic": "BERT"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:6-12",
            "topic": "[12]"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=1",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Timeline_of_machine_learning",
            "topic": "Timeline of machine learning"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=2",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Recurrent_neural_network",
            "topic": "recurrent neural networks"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Elman_network",
            "topic": "Elman network"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Vanishing-gradient_problem",
            "topic": "vanishing-gradient problem"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Long_short-term_memory",
            "topic": "LSTM"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-13",
            "topic": "[note 1]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Attention_mechanism",
            "topic": "attention mechanism"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-14",
            "topic": "[13]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-PDP-15",
            "topic": "[14]"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Higher-order_neural_network&action=edit&redlink=1",
            "topic": "higher-order networks"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-16",
            "topic": "[15]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-17",
            "topic": "[note 2]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Quadratic_function",
            "topic": "quadratic"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Fast_weight&action=edit&redlink=1",
            "topic": "fast weight"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-transform19922-18",
            "topic": "[16]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-malsburg1981-19",
            "topic": "[17]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-feldman1982-20",
            "topic": "[18]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-21",
            "topic": "[19]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-transform19922-18",
            "topic": "[16]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-fastlinear20202-22",
            "topic": "[20]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-schlag20212-23",
            "topic": "[21]"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=3",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Seq2seq#History",
            "topic": "Seq2seq § History"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:22-24",
            "topic": "[22]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-sequence-25",
            "topic": "[23]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:22-24",
            "topic": "[22]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-sequence-25",
            "topic": "[23]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Long_short-term_memory",
            "topic": "long short-term memories"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-sequence-25",
            "topic": "[23]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Gated_recurrent_unit",
            "topic": "gated recurrent units"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:22-24",
            "topic": "[22]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-MyUser_Arxiv.org_May_18_2016c-26",
            "topic": "[24]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-gruber_jockisch-27",
            "topic": "[25]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-28",
            "topic": "[26]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-inventors-4",
            "topic": "[4]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-29",
            "topic": "[27]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Google_Translate",
            "topic": "Google Translate"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Google_Neural_Machine_Translation",
            "topic": "Google Neural Machine Translation"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Statistical_machine_translation",
            "topic": "statistical machine translation"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-Y4moj-30",
            "topic": "[28]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-UJDu8-31",
            "topic": "[29]"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=4",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Attention_(machine_learning)#History",
            "topic": "Attention (machine learning) § History"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Parallel_computing",
            "topic": "parallelize"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Feedforward_neural_network",
            "topic": "feedforward networks"
        },
        {
            "link": "https://en.wikipedia.org/wiki/State_of_the_art",
            "topic": "SOTA"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Textual_entailment",
            "topic": "textual entailment"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-32",
            "topic": "[30]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:11-33",
            "topic": "[31]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:11-33",
            "topic": "[31]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-34",
            "topic": "[32]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Attention_is_all_you_need",
            "topic": "Attention is all you need"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Seq2seq",
            "topic": "seq2seq"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Machine_translation",
            "topic": "machine translation"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-2017_Attention_Is_All_You_Need-1",
            "topic": "[1]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-35",
            "topic": "[33]"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=5",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-36",
            "topic": "[34]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Generative_artificial_intelligence",
            "topic": "generative models"
        },
        {
            "link": "https://en.wikipedia.org/wiki/AI_boom",
            "topic": "AI boom"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ELMo",
            "topic": "ELMo"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Word_embedding",
            "topic": "word embeddings"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Bag-of-words_model",
            "topic": "bag of words"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Word2vec",
            "topic": "word2vec"
        },
        {
            "link": "https://en.wikipedia.org/wiki/BERT_(language_model)",
            "topic": "BERT"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:03-37",
            "topic": "[35]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-38",
            "topic": "[36]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-39",
            "topic": "[37]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Generative_pre-trained_transformer",
            "topic": "GPT series"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Natural_language_generation",
            "topic": "natural language generation"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ChatGPT",
            "topic": "ChatGPT"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-40",
            "topic": "[38]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Large_language_model",
            "topic": "large language models"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-gpt12-41",
            "topic": "[39]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-ngEG3-42",
            "topic": "[40]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Vision_transformer",
            "topic": "vision transformer"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-auto2-43",
            "topic": "[41]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-Gulati2020-44",
            "topic": "[42]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:10-6",
            "topic": "[6]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Multimodal_learning",
            "topic": "multimodal"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-choromanski2020-45",
            "topic": "[43]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Convolutional_neural_network",
            "topic": "convolutional neural networks"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-46",
            "topic": "[44]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/DALL-E",
            "topic": "DALL-E"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Stable_Diffusion",
            "topic": "Stable Diffusion 3"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:62-47",
            "topic": "[45]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Sora_(text-to-video_model)",
            "topic": "Sora"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=6",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=7",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-2017_Attention_Is_All_You_Need-1",
            "topic": "[1]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Layer_normalization",
            "topic": "layer normalization"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-auto1-48",
            "topic": "[46]"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=8",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Self-supervised_learning",
            "topic": "self-supervised learning"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Supervised_learning",
            "topic": "supervised"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)",
            "topic": "fine-tuning"
        },
        {
            "link": "https://en.wikipedia.org/wiki/The_Pile_(dataset)",
            "topic": "The Pile"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Language_modeling",
            "topic": "language modeling"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:6-12",
            "topic": "[12]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:6-12",
            "topic": "[12]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Question_answering",
            "topic": "question answering"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:7-3",
            "topic": "[3]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Natural-language_understanding",
            "topic": "reading comprehension"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Sentiment_analysis",
            "topic": "sentiment analysis"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-2017_Attention_Is_All_You_Need-1",
            "topic": "[1]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Text_Summaries",
            "topic": "paraphrasing"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-2017_Attention_Is_All_You_Need-1",
            "topic": "[1]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/T5_(language_model)",
            "topic": "T5 transformer"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:0-49",
            "topic": "[47]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Natural_language",
            "topic": "natural language"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Machine_translation",
            "topic": "machine translation"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-50",
            "topic": "[48]"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=9",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Large_language_model#Evaluation",
            "topic": "Large language model § Evaluation"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:5-51",
            "topic": "[49]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:8-52",
            "topic": "[50]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:4-53",
            "topic": "[51]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:5-51",
            "topic": "[49]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Loss_function",
            "topic": "loss function"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Perplexity",
            "topic": "log-perplexities"
        },
        {
            "link": "https://en.wikipedia.org/wiki/BERT_(language_model)",
            "topic": "BERT series of models"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:8-52",
            "topic": "[50]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Generative_pre-trained_transformer",
            "topic": "GPT series of models"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:4-53",
            "topic": "[51]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/T5_(language_model)",
            "topic": "T5 series of models"
        },
        {
            "link": "https://en.wikipedia.org#Masked_attention",
            "topic": "masked attention"
        },
        {
            "link": "https://en.wikipedia.org#prefixLM",
            "topic": "\"prefixLM\" (prefix language model)"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=10",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org#Subsequent_work",
            "topic": "following section"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=11",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Tokenization",
            "topic": "Tokenization"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Lexical_analysis",
            "topic": "Lexical analysis"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Lexical_analysis",
            "topic": "tokenizer"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Byte_pair_encoding",
            "topic": "byte pair encoding"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=12",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Word_embedding",
            "topic": "Word embedding"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Lookup_table",
            "topic": "lookup table"
        },
        {
            "link": "https://en.wikipedia.org/wiki/One-hot",
            "topic": "one-hot"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:03-37",
            "topic": "[35]"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=13",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Softmax_function",
            "topic": "softmax"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-54",
            "topic": "[52]"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=14",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Sine_wave",
            "topic": "sinusoidal"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Bag-of-words_model",
            "topic": "bag of words"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Man_bites_dog",
            "topic": "man bites dog"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Integer",
            "topic": "integer"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-2017_Attention_Is_All_You_Need-1",
            "topic": "[1]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Convolutional_neural_network",
            "topic": "convolutional neural network"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Language_model",
            "topic": "language model"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Complex_number#Matrix_representation_of_complex_numbers",
            "topic": "complex multiplication can be implemented as real 2-by-2 matrix multiplication"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=15",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Seq2seq",
            "topic": "seq2seq"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-55",
            "topic": "[53]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:1-56",
            "topic": "[54]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Feedforward_neural_network",
            "topic": "feed-forward neural network"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:1-56",
            "topic": "[54]"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=16",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Feedforward_neural_network",
            "topic": "multilayer perceptrons"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Rectifier_(neural_networks)",
            "topic": "ReLU"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-57",
            "topic": "[55]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:03-37",
            "topic": "[35]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:03-37",
            "topic": "[35]"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=17",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Dot-product_attention",
            "topic": "Dot-product attention"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=18",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Dot_product",
            "topic": "dot-product"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Attention_(machine_learning)",
            "topic": "attention"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Dot_product",
            "topic": "dot product"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Softmax_function",
            "topic": "softmax"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Softmax_function",
            "topic": "softmax function"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=19",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-58",
            "topic": "[56]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Parallel_computing",
            "topic": "parallel"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Feedforward_neural_network",
            "topic": "feed-forward neural network"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=20",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org/wiki/XLNet",
            "topic": "XLNet"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Permutation_matrix",
            "topic": "permutation matrix"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-59",
            "topic": "[57]"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=21",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=22",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-2017_Attention_Is_All_You_Need-1",
            "topic": "[1]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:1-56",
            "topic": "[54]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-2017_Attention_Is_All_You_Need-1",
            "topic": "[1]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Autoregressive_model",
            "topic": "autoregressive"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=23",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=24",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Object_hierarchy",
            "topic": "object hierarchy"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Object-oriented_programming",
            "topic": "object-oriented programming"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Residual_neural_network",
            "topic": "residual connections"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Layer_normalization",
            "topic": "layer normalization"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-60",
            "topic": "[58]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-auto1-48",
            "topic": "[46]"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=25",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-61",
            "topic": "[59]"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=26",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:3-62",
            "topic": "[60]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Feature_learning",
            "topic": "representation learning"
        },
        {
            "link": "https://en.wikipedia.org/wiki/BERT_(language_model)",
            "topic": "BERT"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:4-53",
            "topic": "[51]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Natural_language_generation",
            "topic": "text generation"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Large_language_model#Instruction_tuning",
            "topic": "instruction following"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Generative_pre-trained_transformer",
            "topic": "GPT series"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Chinchilla_(language_model)",
            "topic": "Chinchilla series"
        },
        {
            "link": "https://en.wikipedia.org#Alternative_activation_functions",
            "topic": "alternative activation functions"
        },
        {
            "link": "https://en.wikipedia.org#pre-LN",
            "topic": "changing the location of normalization"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Natural_language_generation",
            "topic": "text generation"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Large_language_model#Instruction_tuning",
            "topic": "instruction following"
        },
        {
            "link": "https://en.wikipedia.org/wiki/T5_(language_model)",
            "topic": "T5 series"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:3-62",
            "topic": "[60]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:3-62",
            "topic": "[60]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:4-53",
            "topic": "[51]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-63",
            "topic": "[61]"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=27",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=28",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ReLU",
            "topic": "ReLU"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Activation_function",
            "topic": "activation function"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Llama_(language_model)",
            "topic": "Llama series"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-64",
            "topic": "[62]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:03-37",
            "topic": "[35]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-65",
            "topic": "[63]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Gated_Linear_Unit",
            "topic": "Gated Linear Units"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-66",
            "topic": "[64]"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=29",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org/wiki/RMSNorm",
            "topic": "RMSNorm"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-67",
            "topic": "[65]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Llama_(language_model)",
            "topic": "Llama series"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-68",
            "topic": "[66]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:9-69",
            "topic": "[67]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:9-69",
            "topic": "[67]"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=30",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-70",
            "topic": "[68]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-71",
            "topic": "[69]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-2017_Attention_Is_All_You_Need-1",
            "topic": "[1]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-72",
            "topic": "[70]"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=31",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-73",
            "topic": "[71]"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=32",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-74",
            "topic": "[72]"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=33",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-75",
            "topic": "[73]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Toeplitz_matrix",
            "topic": "Toeplitz matrix"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-76",
            "topic": "[74]"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=34",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Framework_(computer_science)",
            "topic": "frameworks"
        },
        {
            "link": "https://en.wikipedia.org/wiki/TensorFlow",
            "topic": "TensorFlow"
        },
        {
            "link": "https://en.wikipedia.org/wiki/PyTorch",
            "topic": "PyTorch"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Hugging_Face",
            "topic": "Hugging Face"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-wolf2020-11",
            "topic": "[11]"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=35",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-77",
            "topic": "[75]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Block_matrix#Block_matrix_operations",
            "topic": "matrix multiplications in blocks"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Cache_(computing)",
            "topic": "cache"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-78",
            "topic": "[76]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-79",
            "topic": "[77]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-80",
            "topic": "[78]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Nvidia_A100",
            "topic": "A100"
        },
        {
            "link": "https://en.wikipedia.org/wiki/FP16",
            "topic": "FP16"
        },
        {
            "link": "https://en.wikipedia.org/wiki/BF16",
            "topic": "BF16"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-81",
            "topic": "[79]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Nvidia_H100",
            "topic": "H100"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=36",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-82",
            "topic": "[80]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-83",
            "topic": "[81]"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=37",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Memory_paging",
            "topic": "memory paging"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-84",
            "topic": "[82]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-85",
            "topic": "[83]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-86",
            "topic": "[84]"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=38",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:2-87",
            "topic": "[85]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-88",
            "topic": "[86]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Speculative_execution",
            "topic": "speculative execution"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:2-87",
            "topic": "[85]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-89",
            "topic": "[87]"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=39",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-reformer-90",
            "topic": "[88]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-91",
            "topic": "[89]"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=40",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-reformer-90",
            "topic": "[88]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-92",
            "topic": "[90]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Locality-sensitive_hashing",
            "topic": "locality-sensitive hashing"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-93",
            "topic": "[91]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-94",
            "topic": "[92]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-95",
            "topic": "[93]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Small-world_network",
            "topic": "small-world networks"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-96",
            "topic": "[94]"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=41",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-97",
            "topic": "[95]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Radial_basis_function_kernel#Fourier_random_features",
            "topic": "Fourier random features"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-98",
            "topic": "[96]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process",
            "topic": "Gram-Schmidt processed"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=42",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Transfer_learning",
            "topic": "transfer learning"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-99",
            "topic": "[97]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-100",
            "topic": "[98]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Vision_transformer",
            "topic": "ViT"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-101",
            "topic": "[99]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Vision_transformer",
            "topic": "Vision transformers"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-auto2-43",
            "topic": "[41]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-Gulati2020-44",
            "topic": "[42]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Whisper_(speech_recognition_system)",
            "topic": "Whisper"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-Radford_Kim_Xu_Brockman_p.-102",
            "topic": "[100]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Speech_recognition",
            "topic": "speech recognition"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Spectrogram",
            "topic": "spectrogram"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Perceiver",
            "topic": "Perceivers"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-perceiver2021-103",
            "topic": "[101]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-jaegle2021b-104",
            "topic": "[102]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/DALL-E",
            "topic": "DALL-E 1"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-105",
            "topic": "[103]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:13-106",
            "topic": "[104]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:12-107",
            "topic": "[105]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Variational_autoencoder",
            "topic": "variational autoencoder"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-108",
            "topic": "[106]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-109",
            "topic": "[107]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:12-107",
            "topic": "[105]"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-:13-106",
            "topic": "[104]"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=43",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Natural_language_processing",
            "topic": "natural language processing"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Large_language_model",
            "topic": "large language models"
        },
        {
            "link": "https://en.wikipedia.org/wiki/GPT-2",
            "topic": "GPT-2"
        },
        {
            "link": "https://en.wikipedia.org/wiki/GPT-3",
            "topic": "GPT-3"
        },
        {
            "link": "https://en.wikipedia.org/wiki/GPT-4",
            "topic": "GPT-4"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=AlbertAGPT&action=edit&redlink=1",
            "topic": "AlbertAGPT"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Anthropic#Claude",
            "topic": "Claude"
        },
        {
            "link": "https://en.wikipedia.org/wiki/BERT_(language_model)",
            "topic": "BERT"
        },
        {
            "link": "https://en.wikipedia.org/wiki/XLNet",
            "topic": "XLNet"
        },
        {
            "link": "https://en.wikipedia.org/wiki/BERT_(language_model)#RoBERTa",
            "topic": "RoBERTa"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ChatGPT",
            "topic": "ChatGPT"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Machine_translation",
            "topic": "machine translation"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Time_series",
            "topic": "time series"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Automatic_summarization",
            "topic": "document summarization"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Natural_language_generation",
            "topic": "document generation"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Named-entity_recognition",
            "topic": "named entity recognition"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-110",
            "topic": "[108]"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Computer_programming",
            "topic": "writing computer code"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Speech-to-text",
            "topic": "speech-to-text"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Sequence_analysis",
            "topic": "biological sequence analysis"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Computer_vision",
            "topic": "video understanding"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Protein_structure_prediction",
            "topic": "protein folding"
        },
        {
            "link": "https://en.wikipedia.org/wiki/AlphaFold",
            "topic": "AlphaFold"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Evaluation_function",
            "topic": "evaluating"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Minimax",
            "topic": "Minimax"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Elo_rating_system",
            "topic": "Elo"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Grandmaster_(chess)",
            "topic": "grandmaster"
        },
        {
            "link": "https://en.wikipedia.org#cite_note-grandmaster-10",
            "topic": "[10]"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=44",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Seq2seq",
            "topic": "seq2seq"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Perceiver",
            "topic": "Perceiver"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Vision_transformer",
            "topic": "Vision transformer"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Large_language_model",
            "topic": "Large language model"
        },
        {
            "link": "https://en.wikipedia.org/wiki/BERT_(language_model)",
            "topic": "BERT (language model)"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Generative_pre-trained_transformer",
            "topic": "Generative pre-trained transformer"
        },
        {
            "link": "https://en.wikipedia.org/wiki/T5_(language_model)",
            "topic": "T5 (language model)"
        },
        {
            "link": "https://www.mdpi.com/2227-7390/12/22/3506",
            "topic": "Mean-Field-Type Transformers (MFTT)"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=45",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-13",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Gated_recurrent_units",
            "topic": "Gated recurrent units"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-17",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=46",
            "topic": "edit"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-2017_Attention_Is_All_You_Need_1-0",
            "topic": "a"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-2017_Attention_Is_All_You_Need_1-1",
            "topic": "b"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-2017_Attention_Is_All_You_Need_1-2",
            "topic": "c"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-2017_Attention_Is_All_You_Need_1-3",
            "topic": "d"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-2017_Attention_Is_All_You_Need_1-4",
            "topic": "e"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-2017_Attention_Is_All_You_Need_1-5",
            "topic": "f"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-2017_Attention_Is_All_You_Need_1-6",
            "topic": "g"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-2017_Attention_Is_All_You_Need_1-7",
            "topic": "h"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-2017_Attention_Is_All_You_Need_1-8",
            "topic": "i"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-2017_Attention_Is_All_You_Need_1-9",
            "topic": "j"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Ashish_Vaswani",
            "topic": "Vaswani, Ashish"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Aidan_Gomez",
            "topic": "Gomez, Aidan N"
        },
        {
            "link": "https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf",
            "topic": "\"Attention is All you Need\""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-lstm1997_2-0",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Sepp_Hochreiter",
            "topic": "Hochreiter, Sepp"
        },
        {
            "link": "https://en.wikipedia.org/wiki/J%C3%BCrgen_Schmidhuber",
            "topic": "Schmidhuber, Jürgen"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Doi_(identifier)",
            "topic": "doi"
        },
        {
            "link": "https://doi.org/10.1162%2Fneco.1997.9.8.1735",
            "topic": "/neco.1997.9.8.1735"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ISSN_(identifier)",
            "topic": "ISSN"
        },
        {
            "link": "https://search.worldcat.org/issn/0899-7667",
            "topic": "-7667"
        },
        {
            "link": "https://en.wikipedia.org/wiki/PMID_(identifier)",
            "topic": "PMID"
        },
        {
            "link": "https://pubmed.ncbi.nlm.nih.gov/9377276",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org/wiki/S2CID_(identifier)",
            "topic": "S2CID"
        },
        {
            "link": "https://api.semanticscholar.org/CorpusID:1915014",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:7_3-0",
            "topic": "a"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:7_3-1",
            "topic": "b"
        },
        {
            "link": "https://openai.com/blog/better-language-models/",
            "topic": "\"Better Language Models and Their Implications\""
        },
        {
            "link": "https://web.archive.org/web/20201219132206/https://openai.com/blog/better-language-models/",
            "topic": "Archived"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-inventors_4-0",
            "topic": "a"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-inventors_4-1",
            "topic": "b"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/1409.0473",
            "topic": ""
        },
        {
            "link": "https://arxiv.org/archive/cs.CL",
            "topic": "cs.CL"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-inventconfirm_5-0",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/1508.04025",
            "topic": ""
        },
        {
            "link": "https://arxiv.org/archive/cs.CL",
            "topic": "cs.CL"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:10_6-0",
            "topic": "a"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:10_6-1",
            "topic": "b"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/2106.01345",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-7",
            "topic": "^"
        },
        {
            "link": "https://proceedings.mlr.press/v119/parisotto20a.html",
            "topic": "\"Stabilizing Transformers for Reinforcement Learning\""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-Robust_Speech_Recognition_via_Large-Scale_Weak_Supervision_8-0",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/2212.04356",
            "topic": ""
        },
        {
            "link": "https://arxiv.org/archive/eess.AS",
            "topic": "eess.AS"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-9",
            "topic": "^"
        },
        {
            "link": "https://ieeexplore.ieee.org/document/9984828",
            "topic": "\"Learning to Throw With a Handful of Samples Using Decision Transformers\""
        },
        {
            "link": "https://en.wikipedia.org/wiki/Doi_(identifier)",
            "topic": "doi"
        },
        {
            "link": "https://doi.org/10.1109%2FLRA.2022.3229266",
            "topic": "/LRA.2022.3229266"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ISSN_(identifier)",
            "topic": "ISSN"
        },
        {
            "link": "https://search.worldcat.org/issn/2377-3766",
            "topic": "-3766"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-grandmaster_10-0",
            "topic": "a"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-grandmaster_10-1",
            "topic": "b"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/2402.04494v1",
            "topic": "v1"
        },
        {
            "link": "https://arxiv.org/archive/cs.LG",
            "topic": "cs.LG"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-wolf2020_11-0",
            "topic": "a"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-wolf2020_11-1",
            "topic": "b"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Doi_(identifier)",
            "topic": "doi"
        },
        {
            "link": "https://doi.org/10.18653%2Fv1%2F2020.emnlp-demos.6",
            "topic": "/v1/2020.emnlp-demos.6"
        },
        {
            "link": "https://en.wikipedia.org/wiki/S2CID_(identifier)",
            "topic": "S2CID"
        },
        {
            "link": "https://api.semanticscholar.org/CorpusID:208117506",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:6_12-0",
            "topic": "a"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:6_12-1",
            "topic": "b"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:6_12-2",
            "topic": "c"
        },
        {
            "link": "http://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html",
            "topic": "\"Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing\""
        },
        {
            "link": "https://web.archive.org/web/20210113211449/https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html",
            "topic": "Archived"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-14",
            "topic": "^"
        },
        {
            "link": "https://www.sciencedirect.com/science/article/pii/S0364021382800013",
            "topic": "\"Connectionist models and their properties\""
        },
        {
            "link": "https://en.wikipedia.org/wiki/Doi_(identifier)",
            "topic": "doi"
        },
        {
            "link": "https://doi.org/10.1016%2FS0364-0213%2882%2980001-3",
            "topic": "/S0364-0213(82)80001-3"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ISSN_(identifier)",
            "topic": "ISSN"
        },
        {
            "link": "https://search.worldcat.org/issn/0364-0213",
            "topic": "-0213"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-PDP_15-0",
            "topic": "^"
        },
        {
            "link": "https://stanford.edu/~jlmcc/papers/PDP/Chapter2.pdf",
            "topic": "Parallel Distributed Processing, Volume 1: Explorations in the Microstructure of Cognition: Foundations, Chapter 2"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ISBN_(identifier)",
            "topic": "ISBN"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Special:BookSources/978-0-262-68053-0",
            "topic": "-0-262-68053-0"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-16",
            "topic": "^"
        },
        {
            "link": "https://opg.optica.org/abstract.cfm?URI=ao-26-23-4972",
            "topic": "\"Learning, invariance, and generalization in high-order neural networks\""
        },
        {
            "link": "https://en.wikipedia.org/wiki/Doi_(identifier)",
            "topic": "doi"
        },
        {
            "link": "https://doi.org/10.1364%2FAO.26.004972",
            "topic": "/AO.26.004972"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ISSN_(identifier)",
            "topic": "ISSN"
        },
        {
            "link": "https://search.worldcat.org/issn/0003-6935",
            "topic": "-6935"
        },
        {
            "link": "https://en.wikipedia.org/wiki/PMID_(identifier)",
            "topic": "PMID"
        },
        {
            "link": "https://pubmed.ncbi.nlm.nih.gov/20523475",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-transform19922_18-0",
            "topic": "a"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-transform19922_18-1",
            "topic": "b"
        },
        {
            "link": "https://en.wikipedia.org/wiki/J%C3%BCrgen_Schmidhuber",
            "topic": "Schmidhuber, Jürgen"
        },
        {
            "link": "https://archive.org/download/wikipedia-scholarly-sources-corpus/10.1162.zip/10.1162%252Fneco.1992.4.1.131.pdf",
            "topic": "\"Learning to control fast-weight memories: an alternative to recurrent nets\""
        },
        {
            "link": "https://en.wikipedia.org/wiki/Doi_(identifier)",
            "topic": "doi"
        },
        {
            "link": "https://doi.org/10.1162%2Fneco.1992.4.1.131",
            "topic": "/neco.1992.4.1.131"
        },
        {
            "link": "https://en.wikipedia.org/wiki/S2CID_(identifier)",
            "topic": "S2CID"
        },
        {
            "link": "https://api.semanticscholar.org/CorpusID:16683347",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-malsburg1981_19-0",
            "topic": "^"
        },
        {
            "link": "http://cogprints.org/1380/1/vdM_correlation.pdf",
            "topic": "http://cogprints.org/1380/1/vdM_correlation.pdf"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-feldman1982_20-0",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-21",
            "topic": "^"
        },
        {
            "link": "https://escholarship.org/uc/item/0570j1dp",
            "topic": "\"Using Fast Weights to Deblur Old Memories\""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-fastlinear20202_22-0",
            "topic": "^"
        },
        {
            "link": "https://proceedings.mlr.press/v119/katharopoulos20a.html",
            "topic": "\"Transformers are RNNs: Fast autoregressive Transformers with linear attention\""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-schlag20212_23-0",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Juergen_Schmidhuber",
            "topic": "Schmidhuber, Jürgen"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:22_24-0",
            "topic": "a"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:22_24-1",
            "topic": "b"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:22_24-2",
            "topic": "c"
        },
        {
            "link": "https://aclanthology.org/D14-1179",
            "topic": "\"Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation\""
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/1406.1078",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org/wiki/Doi_(identifier)",
            "topic": "doi"
        },
        {
            "link": "https://doi.org/10.3115%2Fv1%2FD14-1179",
            "topic": "/v1/D14-1179"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-sequence_25-0",
            "topic": "a"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-sequence_25-1",
            "topic": "b"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-sequence_25-2",
            "topic": "c"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/1409.3215",
            "topic": ""
        },
        {
            "link": "https://arxiv.org/archive/cs.CL",
            "topic": "cs.CL"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-MyUser_Arxiv.org_May_18_2016c_26-0",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/1412.3555",
            "topic": ""
        },
        {
            "link": "https://arxiv.org/archive/cs.NE",
            "topic": "cs.NE"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-gruber_jockisch_27-0",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Doi_(identifier)",
            "topic": "doi"
        },
        {
            "link": "https://doi.org/10.3389%2Ffrai.2020.00040",
            "topic": "/frai.2020.00040"
        },
        {
            "link": "https://en.wikipedia.org/wiki/PMC_(identifier)",
            "topic": "PMC"
        },
        {
            "link": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7861254",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org/wiki/PMID_(identifier)",
            "topic": "PMID"
        },
        {
            "link": "https://pubmed.ncbi.nlm.nih.gov/33733157",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org/wiki/S2CID_(identifier)",
            "topic": "S2CID"
        },
        {
            "link": "https://api.semanticscholar.org/CorpusID:220252321",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-28",
            "topic": "^"
        },
        {
            "link": "https://proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html",
            "topic": "\"Sequence to Sequence Learning with Neural Networks\""
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/1409.3215",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-29",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/1508.04025",
            "topic": ""
        },
        {
            "link": "https://arxiv.org/archive/cs.CL",
            "topic": "cs.CL"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-Y4moj_30-0",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/1609.08144",
            "topic": ""
        },
        {
            "link": "https://arxiv.org/archive/cs.CL",
            "topic": "cs.CL"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-UJDu8_31-0",
            "topic": "^"
        },
        {
            "link": "https://web.archive.org/web/20230524052626/https://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html",
            "topic": "\"The Great A.I. Awakening\""
        },
        {
            "link": "https://en.wikipedia.org/wiki/ISSN_(identifier)",
            "topic": "ISSN"
        },
        {
            "link": "https://search.worldcat.org/issn/0362-4331",
            "topic": "-4331"
        },
        {
            "link": "https://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html",
            "topic": "the original"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-32",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/1606.01933",
            "topic": ""
        },
        {
            "link": "https://arxiv.org/archive/cs.CL",
            "topic": "cs.CL"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:11_33-0",
            "topic": "a"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:11_33-1",
            "topic": "b"
        },
        {
            "link": "https://www.wired.com/story/eight-google-employees-invented-modern-ai-transformers-paper/",
            "topic": "\"8 Google Employees Invented Modern AI. Here's the Inside Story\""
        },
        {
            "link": "https://en.wikipedia.org/wiki/ISSN_(identifier)",
            "topic": "ISSN"
        },
        {
            "link": "https://search.worldcat.org/issn/1059-1028",
            "topic": "-1028"
        },
        {
            "link": "https://web.archive.org/web/20240320101528/https://www.wired.com/story/eight-google-employees-invented-modern-ai-transformers-paper/",
            "topic": "Archived"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-34",
            "topic": "^"
        },
        {
            "link": "https://aclanthology.org/D16-1053/",
            "topic": "\"Long Short-Term Memory-Networks for Machine Reading\""
        },
        {
            "link": "https://en.wikipedia.org/wiki/Doi_(identifier)",
            "topic": "doi"
        },
        {
            "link": "https://doi.org/10.18653%2Fv1%2FD16-1053",
            "topic": "/v1/D16-1053"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-35",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/2305.13048",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-36",
            "topic": "^"
        },
        {
            "link": "https://www.newyorker.com/science/annals-of-artificial-intelligence/was-linguistic-ai-created-by-accident",
            "topic": "\"Was Linguistic A.I. Created by Accident?\""
        },
        {
            "link": "https://en.wikipedia.org/wiki/ISSN_(identifier)",
            "topic": "ISSN"
        },
        {
            "link": "https://search.worldcat.org/issn/0028-792X",
            "topic": "-792X"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:03_37-0",
            "topic": "a"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:03_37-1",
            "topic": "b"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:03_37-2",
            "topic": "c"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:03_37-3",
            "topic": "d"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:03_37-4",
            "topic": "e"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/1810.04805v2",
            "topic": "v2"
        },
        {
            "link": "https://arxiv.org/archive/cs.CL",
            "topic": "cs.CL"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-38",
            "topic": "^"
        },
        {
            "link": "https://searchengineland.com/google-bert-used-on-almost-every-english-query-342193",
            "topic": "\"Google: BERT now used on almost every English query\""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-39",
            "topic": "^"
        },
        {
            "link": "https://research.google/blog/recent-advances-in-google-translate/",
            "topic": "\"Recent Advances in Google Translate\""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-40",
            "topic": "^"
        },
        {
            "link": "https://www.technologyreview.com/2023/03/03/1069311/inside-story-oral-history-how-chatgpt-built-openai/",
            "topic": "\"The inside story of how ChatGPT was built from the people who made it\""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-gpt12_41-0",
            "topic": "^"
        },
        {
            "link": "https://openai.com/research/language-unsupervised",
            "topic": "\"Improving language understanding with unsupervised learning\""
        },
        {
            "link": "https://web.archive.org/web/20230318210736/https://openai.com/research/language-unsupervised",
            "topic": "Archived"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-ngEG3_42-0",
            "topic": "^"
        },
        {
            "link": "https://github.com/openai/finetune-transformer-lm",
            "topic": "finetune-transformer-lm"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-auto2_43-0",
            "topic": "a"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-auto2_43-1",
            "topic": "b"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/2010.11929",
            "topic": ""
        },
        {
            "link": "https://arxiv.org/archive/cs.CV",
            "topic": "cs.CV"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-Gulati2020_44-0",
            "topic": "a"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-Gulati2020_44-1",
            "topic": "b"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/2005.08100",
            "topic": ""
        },
        {
            "link": "https://arxiv.org/archive/eess.AS",
            "topic": "eess.AS"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-choromanski2020_45-0",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/2009.14794",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-46",
            "topic": "^"
        },
        {
            "link": "https://openaccess.thecvf.com/content/CVPR2022/html/Liu_A_ConvNet_for_the_2020s_CVPR_2022_paper.html",
            "topic": "A ConvNet for the 2020s"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:62_47-0",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/2403.03206",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-auto1_48-0",
            "topic": "a"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-auto1_48-1",
            "topic": "b"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/2002.04745",
            "topic": ""
        },
        {
            "link": "https://arxiv.org/archive/cs.LG",
            "topic": "cs.LG"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:0_49-0",
            "topic": "^"
        },
        {
            "link": "https://dl.acm.org/doi/abs/10.5555/3455716.3455856",
            "topic": "\"Exploring the limits of transfer learning with a unified text-to-text transformer\""
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/1910.10683",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org/wiki/ISSN_(identifier)",
            "topic": "ISSN"
        },
        {
            "link": "https://search.worldcat.org/issn/1532-4435",
            "topic": "-4435"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-50",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/1910.10683",
            "topic": ""
        },
        {
            "link": "https://arxiv.org/archive/cs.LG",
            "topic": "cs.LG"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:5_51-0",
            "topic": "a"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:5_51-1",
            "topic": "b"
        },
        {
            "link": "https://huggingface.co/docs/transformers/tasks/masked_language_modeling",
            "topic": "\"Masked language modeling\""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:8_52-0",
            "topic": "a"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:8_52-1",
            "topic": "b"
        },
        {
            "link": "https://huggingface.co/docs/transformers/tasks/language_modeling",
            "topic": "\"Causal language modeling\""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:4_53-0",
            "topic": "a"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:4_53-1",
            "topic": "b"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:4_53-2",
            "topic": "c"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:4_53-3",
            "topic": "d"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/2205.05131",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-54",
            "topic": "^"
        },
        {
            "link": "https://arxiv.org/abs/1608.05859",
            "topic": "Using the Output Embedding to Improve Language Models"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/1608.05859",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-55",
            "topic": "^"
        },
        {
            "link": "https://indico.io/blog/sequence-modeling-neural-networks-part2-attention-models/",
            "topic": "\"Sequence Modeling with Neural Networks (Part 2): Attention Models\""
        },
        {
            "link": "https://web.archive.org/web/20201021203352/https://indico.io/blog/sequence-modeling-neural-networks-part2-attention-models/",
            "topic": "Archived"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:1_56-0",
            "topic": "a"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:1_56-1",
            "topic": "b"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:1_56-2",
            "topic": "c"
        },
        {
            "link": "http://jalammar.github.io/illustrated-transformer/",
            "topic": "\"The Illustrated Transformer\""
        },
        {
            "link": "https://web.archive.org/web/20201018061610/https://jalammar.github.io/illustrated-transformer/",
            "topic": "Archived"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-57",
            "topic": "^"
        },
        {
            "link": "https://keras.io/api/keras_nlp/models/gpt2/gpt2_backbone/",
            "topic": "\"Keras documentation: GPT2Backbone model\""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-58",
            "topic": "^"
        },
        {
            "link": "https://www.aclweb.org/anthology/W19-4828",
            "topic": "\"What Does BERT Look at? An Analysis of BERT's Attention\""
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/1906.04341",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org/wiki/Doi_(identifier)",
            "topic": "doi"
        },
        {
            "link": "https://doi.org/10.18653%2Fv1%2FW19-4828",
            "topic": "/v1/W19-4828"
        },
        {
            "link": "https://web.archive.org/web/20201021211357/https://www.aclweb.org/anthology/W19-4828/",
            "topic": "Archived"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-59",
            "topic": "^"
        },
        {
            "link": "https://proceedings.neurips.cc/paper/2019/hash/dc6a7e655d7e5840e66733e9ee67cc69-Abstract.html",
            "topic": "\"XLNet: Generalized Autoregressive Pretraining for Language Understanding\""
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/1906.08237",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-60",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/1906.01787",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-61",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/2207.09238",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:3_62-0",
            "topic": "a"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:3_62-1",
            "topic": "b"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:3_62-2",
            "topic": "c"
        },
        {
            "link": "http://jmlr.org/papers/v21/20-074.html",
            "topic": "\"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\""
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/1910.10683",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org/wiki/ISSN_(identifier)",
            "topic": "ISSN"
        },
        {
            "link": "https://search.worldcat.org/issn/1533-7928",
            "topic": "-7928"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-63",
            "topic": "^"
        },
        {
            "link": "https://research.google/blog/recent-advances-in-google-translate/",
            "topic": "\"Recent Advances in Google Translate\""
        },
        {
            "link": "https://web.archive.org/web/20240704042433/https://research.google/blog/recent-advances-in-google-translate/",
            "topic": "Archived"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-64",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/2002.05202",
            "topic": ""
        },
        {
            "link": "https://arxiv.org/archive/cs.LG",
            "topic": "cs.LG"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-65",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/1606.08415v5",
            "topic": "v5"
        },
        {
            "link": "https://arxiv.org/archive/cs.LG",
            "topic": "cs.LG"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-66",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/2002.05202",
            "topic": ""
        },
        {
            "link": "https://arxiv.org/archive/cs.LG",
            "topic": "cs.LG"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-67",
            "topic": "^"
        },
        {
            "link": "https://proceedings.neurips.cc/paper/2019/hash/1e8a19426224ca89e83cef47f1e7f53b-Abstract.html",
            "topic": "\"Root Mean Square Layer Normalization\""
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/1910.07467",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-68",
            "topic": "^"
        },
        {
            "link": "https://doi.org/10.3390/math12223506",
            "topic": "https://doi.org/10.3390/math12223506"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:9_69-0",
            "topic": "a"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:9_69-1",
            "topic": "b"
        },
        {
            "link": "https://aclanthology.org/2019.iwslt-1.17",
            "topic": "\"Transformers without Tears: Improving the Normalization of Self-Attention\""
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/1910.05895",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org/wiki/Doi_(identifier)",
            "topic": "doi"
        },
        {
            "link": "https://doi.org/10.5281%2Fzenodo.3525484",
            "topic": "/zenodo.3525484"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-70",
            "topic": "^"
        },
        {
            "link": "https://doi.org/10.1162%2Fcoli_a_00445",
            "topic": "\"Position Information in Transformers: An Overview\""
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/2102.11090",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org/wiki/Doi_(identifier)",
            "topic": "doi"
        },
        {
            "link": "https://doi.org/10.1162%2Fcoli_a_00445",
            "topic": "/coli_a_00445"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ISSN_(identifier)",
            "topic": "ISSN"
        },
        {
            "link": "https://search.worldcat.org/issn/0891-2017",
            "topic": "-2017"
        },
        {
            "link": "https://en.wikipedia.org/wiki/S2CID_(identifier)",
            "topic": "S2CID"
        },
        {
            "link": "https://api.semanticscholar.org/CorpusID:231986066",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-71",
            "topic": "^"
        },
        {
            "link": "https://proceedings.mlr.press/v70/gehring17a.html",
            "topic": "\"Convolutional Sequence to Sequence Learning\""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-72",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/2203.16634",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-73",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/2104.09864",
            "topic": ""
        },
        {
            "link": "https://arxiv.org/archive/cs.CL",
            "topic": "cs.CL"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-74",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/2108.12409",
            "topic": ""
        },
        {
            "link": "https://arxiv.org/archive/cs.CL",
            "topic": "cs.CL"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-75",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/1803.02155",
            "topic": ""
        },
        {
            "link": "https://arxiv.org/archive/cs.CL",
            "topic": "cs.CL"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-76",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/2006.15595",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-77",
            "topic": "^"
        },
        {
            "link": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html",
            "topic": "\"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\""
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/2205.14135",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-78",
            "topic": "^"
        },
        {
            "link": "https://crfm.stanford.edu/2023/07/17/flash2.html",
            "topic": "\"Stanford CRFM\""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-79",
            "topic": "^"
        },
        {
            "link": "https://princeton-nlp.github.io/flash-atttention-2/",
            "topic": "\"FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning\""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-80",
            "topic": "^"
        },
        {
            "link": "https://together.ai/blog/tri-dao-flash-attention",
            "topic": "\"Introducing Together AI Chief Scientist Tri Dao, as he releases FlashAttention-2 to speed up model training and inference\""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-81",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/2305.13245",
            "topic": ""
        },
        {
            "link": "https://arxiv.org/archive/cs.CL",
            "topic": "cs.CL"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-82",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/2204.02311",
            "topic": ""
        },
        {
            "link": "https://arxiv.org/archive/cs.CL",
            "topic": "cs.CL"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-83",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/2305.13245",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-84",
            "topic": "^"
        },
        {
            "link": "https://dl.acm.org/doi/10.1145/3600006.3613165",
            "topic": "\"Efficient Memory Management for Large Language Model Serving with PagedAttention\""
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/2309.06180",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org/wiki/Doi_(identifier)",
            "topic": "doi"
        },
        {
            "link": "https://doi.org/10.1145%2F3600006.3613165",
            "topic": "/3600006.3613165"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ISBN_(identifier)",
            "topic": "ISBN"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Special:BookSources/979-8-4007-0229-7",
            "topic": "-8-4007-0229-7"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-85",
            "topic": "^"
        },
        {
            "link": "https://github.com/vllm-project/vllm",
            "topic": "vllm-project/vllm"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-86",
            "topic": "^"
        },
        {
            "link": "https://blog.vllm.ai/2023/06/20/vllm.html",
            "topic": "\"vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention\""
        },
        {
            "link": "https://en.wikipedia.org/wiki/Template:Cite_web",
            "topic": "cite web"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Category:CS1_maint:_multiple_names:_authors_list",
            "topic": "link"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:2_87-0",
            "topic": "a"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:2_87-1",
            "topic": "b"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/2211.17192",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-88",
            "topic": "^"
        },
        {
            "link": "https://yaofu.notion.site/Towards-100x-Speedup-Full-Stack-Transformer-Inference-Optimization-43124c3688e14cffaf2f1d6cbdf26c6c",
            "topic": "\"Towards 100x Speedup: Full Stack Transformer Inference Optimization\""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-89",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/2302.01318",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-reformer_90-0",
            "topic": "a"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-reformer_90-1",
            "topic": "b"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/2001.04451",
            "topic": ""
        },
        {
            "link": "https://arxiv.org/archive/cs.LG",
            "topic": "cs.LG"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-91",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/2011.04006",
            "topic": ""
        },
        {
            "link": "https://arxiv.org/archive/cs.LG",
            "topic": "cs.LG"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-92",
            "topic": "^"
        },
        {
            "link": "http://ai.googleblog.com/2020/01/reformer-efficient-transformer.html",
            "topic": "\"Reformer: The Efficient Transformer\""
        },
        {
            "link": "https://web.archive.org/web/20201022210019/https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html",
            "topic": "Archived"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-93",
            "topic": "^"
        },
        {
            "link": "https://proceedings.neurips.cc/paper/2017/hash/f9be311e65d81a9ad8150a60844bb94c-Abstract.html",
            "topic": "\"The Reversible Residual Network: Backpropagation Without Storing Activations\""
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/1707.04585",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-94",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/1904.10509",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-95",
            "topic": "^"
        },
        {
            "link": "https://ai.googleblog.com/2021/03/constructing-transformers-for-longer.html",
            "topic": "\"Constructing Transformers For Longer Sequences with Sparse Attention Methods\""
        },
        {
            "link": "https://web.archive.org/web/20210918150757/https://ai.googleblog.com/2021/03/constructing-transformers-for-longer.html",
            "topic": "Archived"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-96",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/2105.14103",
            "topic": ""
        },
        {
            "link": "https://arxiv.org/archive/cs.LG",
            "topic": "cs.LG"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-97",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/2103.02143",
            "topic": ""
        },
        {
            "link": "https://arxiv.org/archive/cs.CL",
            "topic": "cs.CL"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-98",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/2006.03555",
            "topic": ""
        },
        {
            "link": "https://arxiv.org/archive/cs.LG",
            "topic": "cs.LG"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-99",
            "topic": "^"
        },
        {
            "link": "https://ojs.aaai.org/index.php/AAAI/article/view/20729",
            "topic": "\"Frozen Pretrained Transformers as Universal Computation Engines\""
        },
        {
            "link": "https://en.wikipedia.org/wiki/Doi_(identifier)",
            "topic": "doi"
        },
        {
            "link": "https://doi.org/10.1609%2Faaai.v36i7.20729",
            "topic": "/aaai.v36i7.20729"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ISSN_(identifier)",
            "topic": "ISSN"
        },
        {
            "link": "https://search.worldcat.org/issn/2374-3468",
            "topic": "-3468"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-100",
            "topic": "^"
        },
        {
            "link": "https://lmsys.org/blog/2023-03-30-vicuna",
            "topic": "\"Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality | LMSYS Org\""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-101",
            "topic": "^"
        },
        {
            "link": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/6dcf277ea32ce3288914faf369fe6de0-Abstract-Conference.html",
            "topic": "\"Visual Instruction Tuning\""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-Radford_Kim_Xu_Brockman_p._102-0",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/2212.04356",
            "topic": ""
        },
        {
            "link": "https://arxiv.org/archive/eess.AS",
            "topic": "eess.AS"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-perceiver2021_103-0",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/2103.03206",
            "topic": ""
        },
        {
            "link": "https://arxiv.org/archive/cs.CV",
            "topic": "cs.CV"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-jaegle2021b_104-0",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/2107.14795",
            "topic": ""
        },
        {
            "link": "https://arxiv.org/archive/cs.LG",
            "topic": "cs.LG"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-105",
            "topic": "^"
        },
        {
            "link": "https://sites.research.google/parti/",
            "topic": "\"Parti: Pathways Autoregressive Text-to-Image Model\""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:13_106-0",
            "topic": "a"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:13_106-1",
            "topic": "b"
        },
        {
            "link": "https://openreview.net/forum?id=vOEXS39nOF",
            "topic": "\"Phenaki: Variable Length Video Generation from Open Domain Textual Descriptions\""
        },
        {
            "link": "https://en.wikipedia.org/wiki/Template:Cite_journal",
            "topic": "cite journal"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Help:CS1_errors#missing_periodical",
            "topic": "help"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:12_107-0",
            "topic": "a"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-:12_107-1",
            "topic": "b"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/2301.00704",
            "topic": ""
        },
        {
            "link": "https://arxiv.org/archive/cs.CV",
            "topic": "cs.CV"
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-108",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/2102.12092",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-109",
            "topic": "^"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/2206.10789",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org#cite_ref-110",
            "topic": "^"
        },
        {
            "link": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9972634",
            "topic": "\"Precision information extraction for rare disease epidemiology at scale\""
        },
        {
            "link": "https://en.wikipedia.org/wiki/Doi_(identifier)",
            "topic": "doi"
        },
        {
            "link": "https://doi.org/10.1186%2Fs12967-023-04011-y",
            "topic": "/s12967-023-04011-y"
        },
        {
            "link": "https://en.wikipedia.org/wiki/PMC_(identifier)",
            "topic": "PMC"
        },
        {
            "link": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9972634",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org/wiki/PMID_(identifier)",
            "topic": "PMID"
        },
        {
            "link": "https://pubmed.ncbi.nlm.nih.gov/36855134",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&action=edit&section=47",
            "topic": "edit"
        },
        {
            "link": "https://nlp.seas.harvard.edu/2018/04/03/attention.html",
            "topic": "The Annotated transformer"
        },
        {
            "link": "https://web.archive.org/web/20210922093841/https://nlp.seas.harvard.edu/2018/04/03/attention.html",
            "topic": "Archived"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Wayback_Machine",
            "topic": "Wayback Machine"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/2207.09238",
            "topic": ""
        },
        {
            "link": "https://arxiv.org/archive/cs.LG",
            "topic": "cs.LG"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
            "topic": "arXiv"
        },
        {
            "link": "https://arxiv.org/abs/2405.00208",
            "topic": ""
        },
        {
            "link": "https://arxiv.org/archive/cs.CL",
            "topic": "cs.CL"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Template:Google_AI",
            "topic": "v"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Template_talk:Google_AI",
            "topic": "t"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Special:EditPage/Template:Google_AI",
            "topic": "e"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Google_AI",
            "topic": "Google AI"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Google",
            "topic": "Google"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Google_Brain",
            "topic": "Google Brain"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Google_DeepMind",
            "topic": "Google DeepMind"
        },
        {
            "link": "https://en.wikipedia.org/wiki/AlphaGo",
            "topic": "AlphaGo"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Master_(software)",
            "topic": "Master"
        },
        {
            "link": "https://en.wikipedia.org/wiki/AlphaGo_Zero",
            "topic": "AlphaGo Zero"
        },
        {
            "link": "https://en.wikipedia.org/wiki/AlphaZero",
            "topic": "AlphaZero"
        },
        {
            "link": "https://en.wikipedia.org/wiki/MuZero",
            "topic": "MuZero"
        },
        {
            "link": "https://en.wikipedia.org/wiki/AlphaGo_versus_Fan_Hui",
            "topic": "Fan Hui"
        },
        {
            "link": "https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol",
            "topic": "Lee Sedol"
        },
        {
            "link": "https://en.wikipedia.org/wiki/AlphaGo_versus_Ke_Jie",
            "topic": "Ke Jie"
        },
        {
            "link": "https://en.wikipedia.org/wiki/AlphaGo_(film)",
            "topic": "AlphaGo"
        },
        {
            "link": "https://en.wikipedia.org/wiki/The_MANIAC",
            "topic": "The MANIAC"
        },
        {
            "link": "https://en.wikipedia.org/wiki/AlphaFold",
            "topic": "AlphaFold"
        },
        {
            "link": "https://en.wikipedia.org/wiki/AlphaStar_(software)",
            "topic": "AlphaStar"
        },
        {
            "link": "https://en.wikipedia.org/wiki/AlphaDev",
            "topic": "AlphaDev"
        },
        {
            "link": "https://en.wikipedia.org/wiki/AlphaGeometry",
            "topic": "AlphaGeometry"
        },
        {
            "link": "https://en.wikipedia.org/wiki/WaveNet",
            "topic": "WaveNet"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Gato_(DeepMind)",
            "topic": "Gato"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Quantum_Artificial_Intelligence_Lab",
            "topic": "Quantum Artificial Intelligence Lab"
        },
        {
            "link": "https://en.wikipedia.org/wiki/TensorFlow",
            "topic": "TensorFlow"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Tensor_Processing_Unit",
            "topic": "Tensor Processing Unit"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Google_Assistant",
            "topic": "Assistant"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Sparrow_(chatbot)",
            "topic": "Sparrow"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Gemini_(chatbot)",
            "topic": "Gemini"
        },
        {
            "link": "https://en.wikipedia.org/wiki/BERT_(language_model)",
            "topic": "BERT"
        },
        {
            "link": "https://en.wikipedia.org/wiki/T5_(language_model)",
            "topic": "T5"
        },
        {
            "link": "https://en.wikipedia.org/wiki/LaMDA",
            "topic": "LaMDA"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Chinchilla_(language_model)",
            "topic": "Chinchilla"
        },
        {
            "link": "https://en.wikipedia.org/wiki/PaLM",
            "topic": "PaLM"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Gemini_(language_model)",
            "topic": "Gemini"
        },
        {
            "link": "https://en.wikipedia.org/wiki/VideoPoet",
            "topic": "VideoPoet"
        },
        {
            "link": "https://en.wikipedia.org/wiki/NotebookLM",
            "topic": "NotebookLM"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Google_Vids",
            "topic": "Vids"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Attention_Is_All_You_Need",
            "topic": "Attention Is All You Need"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Future_of_Go_Summit",
            "topic": "Future of Go Summit"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Generative_pre-trained_transformer",
            "topic": "Generative pre-trained transformer"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Google_Labs",
            "topic": "Google Labs"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Google_Pixel",
            "topic": "Google Pixel"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Google_Workspace",
            "topic": "Google Workspace"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Category:Google_DeepMind",
            "topic": "Category"
        },
        {
            "link": "https://commons.wikimedia.org/wiki/Category:DeepMind",
            "topic": "Commons"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Template:Artificial_intelligence_(AI)",
            "topic": "v"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Template_talk:Artificial_intelligence_(AI)",
            "topic": "t"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Special:EditPage/Template:Artificial_intelligence_(AI)",
            "topic": "e"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Artificial_intelligence",
            "topic": "Artificial intelligence"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Parameter",
            "topic": "Parameter"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)",
            "topic": "Hyperparameter"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Loss_functions_for_classification",
            "topic": "Loss functions"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Regression_analysis",
            "topic": "Regression"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff",
            "topic": "Bias–variance tradeoff"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Double_descent",
            "topic": "Double descent"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Overfitting",
            "topic": "Overfitting"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Cluster_analysis",
            "topic": "Clustering"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Gradient_descent",
            "topic": "Gradient descent"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Stochastic_gradient_descent",
            "topic": "SGD"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Quasi-Newton_method",
            "topic": "Quasi-Newton method"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Conjugate_gradient_method",
            "topic": "Conjugate gradient method"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Backpropagation",
            "topic": "Backpropagation"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Attention_(machine_learning)",
            "topic": "Attention"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Convolution",
            "topic": "Convolution"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Normalization_(machine_learning)",
            "topic": "Normalization"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Batch_normalization",
            "topic": "Batchnorm"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Activation_function",
            "topic": "Activation"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Softmax_function",
            "topic": "Softmax"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Sigmoid_function",
            "topic": "Sigmoid"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Rectifier_(neural_networks)",
            "topic": "Rectifier"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Gating_mechanism",
            "topic": "Gating"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Weight_initialization",
            "topic": "Weight initialization"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Regularization_(mathematics)",
            "topic": "Regularization"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets",
            "topic": "Datasets"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Data_augmentation",
            "topic": "Augmentation"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Prompt_engineering",
            "topic": "Prompt engineering"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Reinforcement_learning",
            "topic": "Reinforcement learning"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Q-learning",
            "topic": "Q-learning"
        },
        {
            "link": "https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action",
            "topic": "SARSA"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Imitation_learning",
            "topic": "Imitation"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Diffusion_process",
            "topic": "Diffusion"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Latent_diffusion_model",
            "topic": "Latent diffusion model"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Autoregressive_model",
            "topic": "Autoregression"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Adversarial_machine_learning",
            "topic": "Adversary"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Retrieval-augmented_generation",
            "topic": "RAG"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback",
            "topic": "RLHF"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Self-supervised_learning",
            "topic": "Self-supervised learning"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Word_embedding",
            "topic": "Word embedding"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)",
            "topic": "Hallucination"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Machine_learning",
            "topic": "Machine learning"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Prompt_engineering#In-context_learning",
            "topic": "In-context learning"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Neural_network_(machine_learning)",
            "topic": "Artificial neural network"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Deep_learning",
            "topic": "Deep learning"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Language_model",
            "topic": "Language model"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Large_language_model",
            "topic": "Large language model"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Neural_machine_translation",
            "topic": "NMT"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
            "topic": "Artificial general intelligence"
        },
        {
            "link": "https://en.wikipedia.org/wiki/AlexNet",
            "topic": "AlexNet"
        },
        {
            "link": "https://en.wikipedia.org/wiki/WaveNet",
            "topic": "WaveNet"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Human_image_synthesis",
            "topic": "Human image synthesis"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Handwriting_recognition",
            "topic": "HWR"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Optical_character_recognition",
            "topic": "OCR"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Deep_learning_speech_synthesis",
            "topic": "Speech synthesis"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ElevenLabs",
            "topic": "ElevenLabs"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Speech_recognition",
            "topic": "Speech recognition"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Whisper_(speech_recognition_system)",
            "topic": "Whisper"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Facial_recognition_system",
            "topic": "Facial recognition"
        },
        {
            "link": "https://en.wikipedia.org/wiki/AlphaFold",
            "topic": "AlphaFold"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Text-to-image_model",
            "topic": "Text-to-image models"
        },
        {
            "link": "https://en.wikipedia.org/wiki/DALL-E",
            "topic": "DALL-E"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Flux_(text-to-image_model)",
            "topic": "Flux"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Ideogram_(text-to-image_model)",
            "topic": "Ideogram"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Midjourney",
            "topic": "Midjourney"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Stable_Diffusion",
            "topic": "Stable Diffusion"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Text-to-video_model",
            "topic": "Text-to-video models"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Sora_(text-to-video_model)",
            "topic": "Sora"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Dream_Machine_(text-to-video_model)",
            "topic": "Dream Machine"
        },
        {
            "link": "https://en.wikipedia.org/wiki/VideoPoet",
            "topic": "VideoPoet"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Music_and_artificial_intelligence",
            "topic": "Music generation"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Suno_AI",
            "topic": "Suno AI"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Udio",
            "topic": "Udio"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Word2vec",
            "topic": "Word2vec"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Seq2seq",
            "topic": "Seq2seq"
        },
        {
            "link": "https://en.wikipedia.org/wiki/GloVe",
            "topic": "GloVe"
        },
        {
            "link": "https://en.wikipedia.org/wiki/BERT_(language_model)",
            "topic": "BERT"
        },
        {
            "link": "https://en.wikipedia.org/wiki/T5_(language_model)",
            "topic": "T5"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Llama_(language_model)",
            "topic": "Llama"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Chinchilla_(language_model)",
            "topic": "Chinchilla AI"
        },
        {
            "link": "https://en.wikipedia.org/wiki/PaLM",
            "topic": "PaLM"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Generative_pre-trained_transformer",
            "topic": "GPT"
        },
        {
            "link": "https://en.wikipedia.org/wiki/GPT-1",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org/wiki/GPT-2",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org/wiki/GPT-3",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org/wiki/GPT-J",
            "topic": "J"
        },
        {
            "link": "https://en.wikipedia.org/wiki/ChatGPT",
            "topic": "ChatGPT"
        },
        {
            "link": "https://en.wikipedia.org/wiki/GPT-4",
            "topic": ""
        },
        {
            "link": "https://en.wikipedia.org/wiki/GPT-4o",
            "topic": "o"
        },
        {
            "link": "https://en.wikipedia.org/wiki/OpenAI_o1",
            "topic": "o1"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Claude_(language_model)",
            "topic": "Claude"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Gemini_(language_model)",
            "topic": "Gemini"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Grok_(chatbot)",
            "topic": "Grok"
        },
        {
            "link": "https://en.wikipedia.org/wiki/LaMDA",
            "topic": "LaMDA"
        },
        {
            "link": "https://en.wikipedia.org/wiki/BLOOM_(language_model)",
            "topic": "BLOOM"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Project_Debater",
            "topic": "Project Debater"
        },
        {
            "link": "https://en.wikipedia.org/wiki/IBM_Watson",
            "topic": "IBM Watson"
        },
        {
            "link": "https://en.wikipedia.org/wiki/IBM_Watsonx",
            "topic": "IBM Watsonx"
        },
        {
            "link": "https://en.wikipedia.org/wiki/IBM_Granite",
            "topic": "Granite"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Huawei_PanGu",
            "topic": "PanGu-Σ"
        },
        {
            "link": "https://en.wikipedia.org/wiki/AlphaGo",
            "topic": "AlphaGo"
        },
        {
            "link": "https://en.wikipedia.org/wiki/AlphaZero",
            "topic": "AlphaZero"
        },
        {
            "link": "https://en.wikipedia.org/wiki/OpenAI_Five",
            "topic": "OpenAI Five"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Self-driving_car",
            "topic": "Self-driving car"
        },
        {
            "link": "https://en.wikipedia.org/wiki/MuZero",
            "topic": "MuZero"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Action_selection",
            "topic": "Action selection"
        },
        {
            "link": "https://en.wikipedia.org/wiki/AutoGPT",
            "topic": "AutoGPT"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Robot_control",
            "topic": "Robot control"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Alan_Turing",
            "topic": "Alan Turing"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Warren_Sturgis_McCulloch",
            "topic": "Warren Sturgis McCulloch"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Walter_Pitts",
            "topic": "Walter Pitts"
        },
        {
            "link": "https://en.wikipedia.org/wiki/John_von_Neumann",
            "topic": "John von Neumann"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Claude_Shannon",
            "topic": "Claude Shannon"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Marvin_Minsky",
            "topic": "Marvin Minsky"
        },
        {
            "link": "https://en.wikipedia.org/wiki/John_McCarthy_(computer_scientist)",
            "topic": "John McCarthy"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Nathaniel_Rochester_(computer_scientist)",
            "topic": "Nathaniel Rochester"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Allen_Newell",
            "topic": "Allen Newell"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Cliff_Shaw",
            "topic": "Cliff Shaw"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Herbert_A._Simon",
            "topic": "Herbert A. Simon"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Oliver_Selfridge",
            "topic": "Oliver Selfridge"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Frank_Rosenblatt",
            "topic": "Frank Rosenblatt"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Bernard_Widrow",
            "topic": "Bernard Widrow"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Joseph_Weizenbaum",
            "topic": "Joseph Weizenbaum"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Seymour_Papert",
            "topic": "Seymour Papert"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Seppo_Linnainmaa",
            "topic": "Seppo Linnainmaa"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Paul_Werbos",
            "topic": "Paul Werbos"
        },
        {
            "link": "https://en.wikipedia.org/wiki/J%C3%BCrgen_Schmidhuber",
            "topic": "Jürgen Schmidhuber"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Yann_LeCun",
            "topic": "Yann LeCun"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Geoffrey_Hinton",
            "topic": "Geoffrey Hinton"
        },
        {
            "link": "https://en.wikipedia.org/wiki/John_Hopfield",
            "topic": "John Hopfield"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Yoshua_Bengio",
            "topic": "Yoshua Bengio"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Lotfi_A._Zadeh",
            "topic": "Lotfi A. Zadeh"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Stephen_Grossberg",
            "topic": "Stephen Grossberg"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Alex_Graves_(computer_scientist)",
            "topic": "Alex Graves"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Andrew_Ng",
            "topic": "Andrew Ng"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Fei-Fei_Li",
            "topic": "Fei-Fei Li"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Alex_Krizhevsky",
            "topic": "Alex Krizhevsky"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Ilya_Sutskever",
            "topic": "Ilya Sutskever"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Demis_Hassabis",
            "topic": "Demis Hassabis"
        },
        {
            "link": "https://en.wikipedia.org/wiki/David_Silver_(computer_scientist)",
            "topic": "David Silver"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Ian_Goodfellow",
            "topic": "Ian Goodfellow"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Andrej_Karpathy",
            "topic": "Andrej Karpathy"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Neural_Turing_machine",
            "topic": "Neural Turing machine"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Differentiable_neural_computer",
            "topic": "Differentiable neural computer"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Vision_transformer",
            "topic": "Vision transformer (ViT)"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Recurrent_neural_network",
            "topic": "Recurrent neural network (RNN)"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Long_short-term_memory",
            "topic": "Long short-term memory (LSTM)"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Gated_recurrent_unit",
            "topic": "Gated recurrent unit (GRU)"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Echo_state_network",
            "topic": "Echo state network"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Multilayer_perceptron",
            "topic": "Multilayer perceptron (MLP)"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Convolutional_neural_network",
            "topic": "Convolutional neural network (CNN)"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Residual_neural_network",
            "topic": "Residual neural network (RNN)"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Highway_network",
            "topic": "Highway network"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Mamba_(deep_learning_architecture)",
            "topic": "Mamba"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Autoencoder",
            "topic": "Autoencoder"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Variational_autoencoder",
            "topic": "Variational autoencoder (VAE)"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Generative_adversarial_network",
            "topic": "Generative adversarial network (GAN)"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Graph_neural_network",
            "topic": "Graph neural network (GNN)"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Portal:Technology",
            "topic": "Technology"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Category:Artificial_neural_networks",
            "topic": "Artificial neural networks"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Category:Machine_learning",
            "topic": "Machine learning"
        },
        {
            "link": "https://en.wikipedia.org/wiki/List_of_artificial_intelligence_companies",
            "topic": "Companies"
        },
        {
            "link": "https://en.wikipedia.org/wiki/List_of_artificial_intelligence_projects",
            "topic": "Projects"
        },
        {
            "link": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&oldid=1260413106",
            "topic": "https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&oldid=1260413106"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Help:Category",
            "topic": "Categories"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Category:Google_software",
            "topic": "Google software"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Category:Neural_network_architectures",
            "topic": "Neural network architectures"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Category:CS1_maint:_multiple_names:_authors_list",
            "topic": "CS1 maint: multiple names: authors list"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Category:CS1_errors:_missing_periodical",
            "topic": "CS1 errors: missing periodical"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Category:Articles_with_short_description",
            "topic": "Articles with short description"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Category:Short_description_is_different_from_Wikidata",
            "topic": "Short description is different from Wikidata"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Category:Webarchive_template_wayback_links",
            "topic": "Webarchive template wayback links"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_4.0_International_License",
            "topic": "Creative Commons Attribution-ShareAlike 4.0 License"
        },
        {
            "link": "https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Terms_of_Use",
            "topic": "Terms of Use"
        },
        {
            "link": "https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy",
            "topic": "Privacy Policy"
        },
        {
            "link": "https://wikimediafoundation.org/",
            "topic": "Wikimedia Foundation, Inc."
        },
        {
            "link": "https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy",
            "topic": "Privacy policy"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Wikipedia:About",
            "topic": "About Wikipedia"
        },
        {
            "link": "https://en.wikipedia.org/wiki/Wikipedia:General_disclaimer",
            "topic": "Disclaimers"
        },
        {
            "link": "https://en.wikipedia.org//en.wikipedia.org/wiki/Wikipedia:Contact_us",
            "topic": "Contact Wikipedia"
        },
        {
            "link": "https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Universal_Code_of_Conduct",
            "topic": "Code of Conduct"
        },
        {
            "link": "https://developer.wikimedia.org",
            "topic": "Developers"
        },
        {
            "link": "https://stats.wikimedia.org/#/en.wikipedia.org",
            "topic": "Statistics"
        },
        {
            "link": "https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Cookie_statement",
            "topic": "Cookie statement"
        },
        {
            "link": "https://en.wikipedia.org//en.m.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&mobileaction=toggle_view_mobile",
            "topic": "Mobile view"
        }
    ]
}